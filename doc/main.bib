
@inproceedings{chen_clustering-based_2005,
	title = {Clustering-based feature selection for verb sense disambiguation},
	url = {https://ieeexplore.ieee.org/document/1598703},
	doi = {10.1109/NLPKE.2005.1598703},
	abstract = {This paper presents a novel feature selection algorithm for supervised verb sense disambiguation. The algorithm disambiguates and aggregates WordNet synsets of a verb's noun phrase (NP) arguments in the training data. It was then used to filter out irrelevant WordNet semantic features introduced by the ambiguity of verb NP arguments. Experimental results showed that our new feature selection method boosted our system's performance on verbs whose meanings depended heavily on their NP arguments. Furthermore, our method outperformed two standard feature selection methods, indicating its effectiveness and advantages, especially for small-sample machine learning tasks like supervised WSD.},
	urldate = {2025-04-25},
	booktitle = {2005 {International} {Conference} on {Natural} {Language} {Processing} and {Knowledge} {Engineering}},
	author = {Chen, J. and Palmer, M.},
	month = oct,
	year = {2005},
	keywords = {Aggregates, Clustering algorithms, Filters, Frequency, Information science, Machine learning, Machine learning algorithms, Smoothing methods, System performance, Training data},
	pages = {36--41},
}

@inproceedings{webson_prompt-based_2022,
	address = {Seattle, United States},
	title = {Do {Prompt}-{Based} {Models} {Really} {Understand} the {Meaning} of {Their} {Prompts}?},
	url = {https://aclanthology.org/2022.naacl-main.167/},
	doi = {10.18653/v1/2022.naacl-main.167},
	abstract = {Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively “good” prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2021). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models' impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.},
	urldate = {2025-04-22},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Webson, Albert and Pavlick, Ellie},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {2300--2344},
}

@inproceedings{ethayarajh_how_2019,
	address = {Hong Kong, China},
	title = {How {Contextual} are {Contextualized} {Word} {Representations}? {Comparing} the {Geometry} of {BERT}, {ELMo}, and {GPT}-2 {Embeddings}},
	shorttitle = {How {Contextual} are {Contextualized} {Word} {Representations}?},
	url = {https://aclanthology.org/D19-1006/},
	doi = {10.18653/v1/D19-1006},
	abstract = {Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5\% of the variance in a word`s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.},
	urldate = {2025-04-22},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Ethayarajh, Kawin},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {55--65},
}

@article{pustejovsky_generative_1991,
	title = {The {Generative} {Lexicon}},
	volume = {17},
	url = {https://aclanthology.org/J91-4003/},
	number = {4},
	urldate = {2025-04-22},
	journal = {Computational Linguistics},
	author = {Pustejovsky, James},
	year = {1991},
	pages = {409--441},
}

@misc{noauthor_llm_nodate,
	title = {{LLM} {Class} — {vLLM}},
	url = {https://docs.vllm.ai/en/latest/api/offline_inference/llm.html},
	urldate = {2025-04-19},
}

@article{piantadosi_reply_2022,
	title = {Reply to {Kodner} et al.: {Fundamental} misunderstanding of both model and methods},
	volume = {119},
	shorttitle = {Reply to {Kodner} et al.},
	url = {https://www.pnas.org/doi/10.1073/pnas.2204944119},
	doi = {10.1073/pnas.2204944119},
	number = {29},
	urldate = {2025-04-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Piantadosi, Steven T. and Yang, Yuan},
	month = jul,
	year = {2022},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2204944119},
}

@article{yang_one_2022,
	title = {One model for the learning of language},
	volume = {119},
	url = {https://www.pnas.org/doi/10.1073/pnas.2021865119},
	doi = {10.1073/pnas.2021865119},
	abstract = {A major goal of linguistics and cognitive science is to understand what class of learning systems can acquire natural language. Until recently, the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space. Here, we describe a learning system that is maximally unconstrained, operating over the space of all computations, and is able to acquire many of the key structures present in natural language from positive evidence alone. We demonstrate this by providing the same learning model with data from 74 distinct formal languages which have been argued to capture key features of language, have been studied in experimental work, or come from an interesting complexity class. The model is able to successfully induce the latent system generating the observed strings from small amounts of evidence in almost all cases, including for regular (e.g., an,  (ab)n , and  \{a,b\}+ ), context-free (e.g.,  anbn, anbn+m , and  xxR ), and context-sensitive (e.g.,  anbncn, anbmcndm , and xx) languages, as well as for many languages studied in learning experiments. These results show that relatively small amounts of positive evidence can support learning of rich classes of generative computations over structures. The model provides an idealized learning setup upon which additional cognitive constraints and biases can be formalized.},
	number = {5},
	urldate = {2025-04-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Yuan and Piantadosi, Steven T.},
	month = feb,
	year = {2022},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2021865119},
}

@article{kodner_another_2022,
	title = {Another model not for the learning of language},
	volume = {119},
	url = {https://www.pnas.org/doi/10.1073/pnas.2204664119},
	doi = {10.1073/pnas.2204664119},
	number = {29},
	urldate = {2025-04-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kodner, Jordan and Caplan, Spencer and Yang, Charles},
	month = jul,
	year = {2022},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2204664119},
}

@misc{li_generation_2025,
	title = {From {Generation} to {Judgment}: {Opportunities} and {Challenges} of {LLM}-as-a-judge},
	shorttitle = {From {Generation} to {Judgment}},
	url = {http://arxiv.org/abs/2411.16594},
	doi = {10.48550/arXiv.2411.16594},
	abstract = {Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the "LLM-as-a-judge" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. Paper list and more resources about LLM-as-a-judge can be found at https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and https://llm-as-a-judge.github.io.},
	urldate = {2025-04-17},
	publisher = {arXiv},
	author = {Li, Dawei and Jiang, Bohan and Huang, Liangjie and Beigi, Alimohammad and Zhao, Chengshuai and Tan, Zhen and Bhattacharjee, Amrita and Jiang, Yuxuan and Chen, Canyu and Wu, Tianhao and Shu, Kai and Cheng, Lu and Liu, Huan},
	month = feb,
	year = {2025},
	note = {arXiv:2411.16594 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{miller_nouns_1990,
	title = {Nouns in {WordNet}: {A} {Lexical} {Inheritance} {System}},
	volume = {3},
	issn = {0950-3846},
	shorttitle = {Nouns in {WordNet}},
	url = {https://doi.org/10.1093/ijl/3.4.245},
	doi = {10.1093/ijl/3.4.245},
	abstract = {Definitions of common nouns typically give a superordinate term plus distinguishing features; that information provides the basis for organizing noun files in WordNet. The superordinate relation (hyponymy) generates a hierarchical semantic organization that is duplicated in the noun files by the use of labeled pointers between sets of synonyms (synsets). The hierarchy is limited in depth, seldom exceeding more than a dozen levels. Distinguishing features are entered in such a way as to create a lexical inheritance system, a system in which each word inherits the distinguishing features of all its superordinates. Three types of distinguishing features are discussed: attributes (modification), parts (meronymy), and functions (predication), but only meronymy is presently implemented in the noun files. Antonymy is also found between nouns, but it is not a fundamental organizing principle for nouns.Coverage is partitioned into twenty-five topical files, each of which deals with a different primitive semantic component. Characteristics of each topic are discussed.},
	number = {4},
	urldate = {2025-04-17},
	journal = {International Journal of Lexicography},
	author = {Miller, George A.},
	month = dec,
	year = {1990},
	pages = {245--264},
}

@misc{davies_word_2008,
	title = {Word frequency: based on one billion word {COCA} corpus},
	url = {https://www.wordfrequency.info/},
	urldate = {2025-04-17},
	author = {Davies, Mark},
	year = {2008},
}

@misc{noauthor_notitle_nodate,
}

@article{miller_introduction_nodate,
	title = {Introduction to {WordNet}: {An} {On}-line {Lexical} {Database}},
	language = {en},
	author = {Miller, George A and Beckwith, Richard and Fellbaum, Christiane and Gross, Derek and Miller, Katherine},
}

@article{bengio_neural_nodate,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difﬁcult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to ﬁght the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a signiﬁcant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach signiﬁcantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
	language = {en},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	urldate = {2025-04-12},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{chronis_method_2023,
	address = {Toronto, Canada},
	title = {A {Method} for {Studying} {Semantic} {Construal} in {Grammatical} {Constructions} with {Interpretable} {Contextual} {Embedding} {Spaces}},
	url = {https://aclanthology.org/2023.acl-long.14/},
	doi = {10.18653/v1/2023.acl-long.14},
	abstract = {We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to automatically derive semantic characterizations of lexical items in two grammatical constructions: nouns in subject or object position within the same sentence, and the AANN construction (e.g., ‘a beautiful three days'). We show that a word in subject position is interpreted as more agentive than the very same word in object position, and that the nouns in the AANN construction are interpreted as more measurement-like than when in the canonical alternation. Our method can probe the distributional meaning of syntactic constructions at a templatic level, abstracted away from specific lexemes.},
	urldate = {2025-04-10},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chronis, Gabriella and Mahowald, Kyle and Erk, Katrin},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {242--261},
}

@article{li_word_2025,
	title = {Word embedding factor based multi-head attention},
	volume = {58},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-025-11115-y},
	doi = {10.1007/s10462-025-11115-y},
	abstract = {The natural language processing (NLP) field has made significant progress using deep learning models based on multi-head attention mechanisms, such as Transformer and BERT. However, there are two major limitations to this approach. First, the number of heads is often manually set based on empirical experience, and second, it is not clear enough in semantic understanding and interpretation. In this study, we propose a novel attention mechanism called Factor Analysis-based Multi-head (FAM) Attention, which combines the theory of explorative factor analysis and word embedding. The experimental results demonstrate that FAM Attention achieves better performance and requires fewer parameters compared to traditional methods while also having better semantic understanding ability and interpretability at the token level. This also has significant implications for current Large Language Models (LLMs), particularly in terms of effectively reducing parameter counts and enhancing performance.},
	language = {en},
	number = {4},
	urldate = {2025-04-10},
	journal = {Artificial Intelligence Review},
	author = {Li, Zhengren and Zhao, Yumeng and Zhang, Xiaohang and Han, Huawei and Huang, Cui},
	month = jan,
	year = {2025},
	keywords = {Artificial Intelligence, BERT, Factor analysis, Multi-head attention, Word embedding},
	pages = {115},
}

@article{zhang_dependence_nodate,
	title = {Dependence of {Clustering} {Algorithm} {Performance} on {Clustered}-ness of {Data}},
	abstract = {Intuitively, clustering algorithms should work better on the datasets that have well separated clusters. But we found the contrary for the center-based clustering algorithms, including K-Means, KHarmonic Means and EM. We generated 1200 synthetic datasets with varying ratio of inter-cluster variance over within-cluster variance, which we call the clustered-ness of the dataset. We run K-Means, K-Harmonic Means and EM on these datasets and found that the ratio of the performance over the global optimum grows with increasing clustered-ness. Dependence of clustering algorithm performance on other parameters -quality of initialization and dimensionality of data -- are also demonstrated.},
	language = {en},
	author = {Zhang, Bin},
}

@article{epter_clusterability_nodate,
	title = {Clusterability {Detection} and {Initial} {Seed} {Selection} in {Large} {Data} {Sets}},
	abstract = {The need for a preliminary assessment of the clustering tendency or clusterability of massive data sets is known. A good clusterability detection method should serve to in uence a decision as to whether to cluster at all, as well as provide useful seed input to a chosen clustering algorithm. We present a framework for the de nition of the clusterability of a data set from a distance-based perspective. We discuss a graphbased system for detecting clusterability and generating seed information including an estimate of the value of k \{\vphantom{\}} the number of clusters in the data set, an input parameter to many distance-based clustering methods. The output of our method is tunable to accommodate a wide variety of clustering methods.},
	language = {en},
	author = {Epter, Scott and Krishnamoorthy, Mukkai and Zaki, Mohammed},
}

@inproceedings{chronis_when_2020,
	address = {Online},
	title = {When is a bishop not like a rook? {When} it`s like a rabbi! {Multi}-prototype {BERT} embeddings for estimating semantic relationships},
	shorttitle = {When is a bishop not like a rook?},
	url = {https://aclanthology.org/2020.conll-1.17/},
	doi = {10.18653/v1/2020.conll-1.17},
	abstract = {This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These embeddings retain contextual knowledge that is critical for some type-level tasks, while being less cumbersome and less subject to outlier effects than exemplar models. Similarity and relatedness estimation, both type-level tasks, benefit from this contextual knowledge, indicating the context-sensitivity of these processes. BERT`s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level phenomena and type-level concreteness ratings. Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness.},
	urldate = {2025-03-27},
	booktitle = {Proceedings of the 24th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Chronis, Gabriella and Erk, Katrin},
	editor = {Fernández, Raquel and Linzen, Tal},
	month = nov,
	year = {2020},
	pages = {227--244},
}

@misc{krishna_understanding_2024,
	title = {Understanding the {Effects} of {Iterative} {Prompting} on {Truthfulness}},
	url = {http://arxiv.org/abs/2402.06625},
	doi = {10.48550/arXiv.2402.06625},
	abstract = {The development of Large Language Models (LLMs) has notably transformed numerous sectors, offering impressive text generation capabilities. Yet, the reliability and truthfulness of these models remain pressing concerns. To this end, we investigate iterative prompting, a strategy hypothesized to refine LLM responses, assessing its impact on LLM truthfulness, an area which has not been thoroughly explored. Our extensive experiments delve into the intricacies of iterative prompting variants, examining their influence on the accuracy and calibration of model responses. Our findings reveal that naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors. In response to these challenges, we introduce several prompting variants designed to address the identified issues. These variants demonstrate marked improvements over existing baselines, signaling a promising direction for future research. Our work provides a nuanced understanding of iterative prompting and introduces novel approaches to enhance the truthfulness of LLMs, thereby contributing to the development of more accurate and trustworthy AI systems.},
	urldate = {2025-03-21},
	publisher = {arXiv},
	author = {Krishna, Satyapriya and Agarwal, Chirag and Lakkaraju, Himabindu},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06625 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{srinivasan_implications_2021,
	title = {The {Implications} of {Polysemy} for {Theories} of {Word} {Learning}},
	volume = {15},
	issn = {1750-8606},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cdep.12411},
	doi = {10.1111/cdep.12411},
	abstract = {Word learning is typically studied as a problem in which children need to learn a single meaning for a new word. And by most theories, children’s learning is itself guided by the assumption that a new word will have only one meaning. However, the majority of words in languages are polysemous, carrying multiple related and distinct meanings. Here, we consider the implications of this disjuncture. As we review, current theories predict that children should struggle to learn polysemous words. And yet research shows that young children readily learn multiple meanings for words and represent them in qualitatively similar ways to adults. Moreover, polysemy may facilitate word learning, by allowing children to use their knowledge of familiar meanings of a word to learn its other meanings. These findings motivate a new perspective on word learning that recognizes polysemy as a fundamental feature of language, instead of treating it as an edge case.},
	language = {en},
	number = {3},
	urldate = {2025-03-19},
	journal = {Child Development Perspectives},
	author = {Srinivasan, Mahesh and Rabagliati, Hugh},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cdep.12411},
	keywords = {ambiguity, language development, lexical semantics, polysemy, word learning},
	pages = {148--153},
}

@inproceedings{lapata_corpus-based_2001,
	title = {A {Corpus}-based {Account} of {Regular} {Polysemy}: {The} {Case} of {Context}-sensitive {Adjectives}},
	shorttitle = {A {Corpus}-based {Account} of {Regular} {Polysemy}},
	url = {https://aclanthology.org/N01-1009/},
	urldate = {2025-03-19},
	booktitle = {Second {Meeting} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	author = {Lapata, Maria},
	year = {2001},
}

@inproceedings{dolan_word_1994,
	title = {Word {Sense} {Ambiguation}: {Clustering} {Related} {Senses}},
	shorttitle = {Word {Sense} {Ambiguation}},
	url = {https://aclanthology.org/C94-2113/},
	urldate = {2025-03-19},
	booktitle = {{COLING} 1994 {Volume} 2: {The} 15th {International} {Conference} on {Computational} {Linguistics}},
	author = {Dolan, William B.},
	year = {1994},
}

@inproceedings{freihat_taxonomic_2016,
	address = {Bucharest, Romania},
	title = {A {Taxonomic} {Classification} of {WordNet} {Polysemy} {Types}},
	url = {https://aclanthology.org/2016.gwc-1.17/},
	abstract = {WordNet represents polysemous terms by capturing the different meanings of these terms at the lexical level, but without giving emphasis on the polysemy types such terms belong to. The state of the art polysemy approaches identify several polysemy types in WordNet but they do not explain how to classify and organize them. In this paper, we present a novel approach for classifying the polysemy types which exploits taxonomic principles which in turn, allow us to discover a set of polysemy structural patterns.},
	urldate = {2025-03-18},
	booktitle = {Proceedings of the 8th {Global} {WordNet} {Conference} ({GWC})},
	publisher = {Global Wordnet Association},
	author = {Freihat, Abed Alhakim and Giunchiglia, Fausto and Dutta, Biswanath},
	editor = {Fellbaum, Christiane and Vossen, Piek and Mititelu, Verginica Barbu and Forascu, Corina},
	year = {2016},
	pages = {106--114},
}

@misc{li_multi-prototypes_2023,
	title = {Multi-{Prototypes} {Convex} {Merging} {Based} {K}-{Means} {Clustering} {Algorithm}},
	url = {http://arxiv.org/abs/2302.07045},
	doi = {10.48550/arXiv.2302.07045},
	abstract = {K-Means algorithm is a popular clustering method. However, it has two limitations: 1) it gets stuck easily in spurious local minima, and 2) the number of clusters k has to be given a priori. To solve these two issues, a multi-prototypes convex merging based K-Means clustering algorithm (MCKM) is presented. First, based on the structure of the spurious local minima of the K-Means problem, a multi-prototypes sampling (MPS) is designed to select the appropriate number of multi-prototypes for data with arbitrary shapes. A theoretical proof is given to guarantee that the multi-prototypes selected by MPS can achieve a constant factor approximation to the optimal cost of the K-Means problem. Then, a merging technique, called convex merging (CM), merges the multi-prototypes to get a better local minima without k being given a priori. Specifically, CM can obtain the optimal merging and estimate the correct k. By integrating these two techniques with K-Means algorithm, the proposed MCKM is an efficient and explainable clustering algorithm for escaping the undesirable local minima of K-Means problem without given k first. Experimental results performed on synthetic and real-world data sets have verified the effectiveness of the proposed algorithm.},
	urldate = {2025-03-16},
	publisher = {arXiv},
	author = {Li, Dong and Zhou, Shuisheng and Zeng, Tieyong and Chan, Raymond H.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07045 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{mccarthy_word_2016,
	title = {Word {Sense} {Clustering} and {Clusterability}},
	volume = {42},
	url = {https://aclanthology.org/J16-2003/},
	doi = {10.1162/COLI_a_00247},
	number = {2},
	urldate = {2025-03-15},
	journal = {Computational Linguistics},
	author = {McCarthy, Diana and Apidianaki, Marianna and Erk, Katrin},
	month = jun,
	year = {2016},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {245--275},
}

@inproceedings{erk_how_2021,
	address = {Online},
	title = {How to marry a star: {Probabilistic} constraints for meaning in context},
	shorttitle = {How to marry a star},
	url = {https://aclanthology.org/2021.scil-1.55/},
	urldate = {2025-03-15},
	booktitle = {Proceedings of the {Society} for {Computation} in {Linguistics} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Erk, Katrin and Herbelot, Aurélie},
	editor = {Ettinger, Allyson and Pavlick, Ellie and Prickett, Brandon},
	month = feb,
	year = {2021},
	pages = {451--453},
}

@inproceedings{chronis_when_2020-1,
	address = {Online},
	title = {When is a bishop not like a rook? {When} it`s like a rabbi! {Multi}-prototype {BERT} embeddings for estimating semantic relationships},
	shorttitle = {When is a bishop not like a rook?},
	url = {https://aclanthology.org/2020.conll-1.17/},
	doi = {10.18653/v1/2020.conll-1.17},
	abstract = {This paper investigates contextual language models, which produce token representations, as a resource for lexical semantics at the word or type level. We construct multi-prototype word embeddings from bert-base-uncased (Devlin et al., 2018). These embeddings retain contextual knowledge that is critical for some type-level tasks, while being less cumbersome and less subject to outlier effects than exemplar models. Similarity and relatedness estimation, both type-level tasks, benefit from this contextual knowledge, indicating the context-sensitivity of these processes. BERT`s token level knowledge also allows the testing of a type-level hypothesis about lexical abstractness, demonstrating the relationship between token-level phenomena and type-level concreteness ratings. Our findings provide important insight into the interpretability of BERT: layer 7 approximates semantic similarity, while the final layer (11) approximates relatedness.},
	urldate = {2025-03-15},
	booktitle = {Proceedings of the 24th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Chronis, Gabriella and Erk, Katrin},
	editor = {Fernández, Raquel and Linzen, Tal},
	month = nov,
	year = {2020},
	pages = {227--244},
}

@article{carston_polysemy_2021,
	title = {Polysemy: {Pragmatics} and sense conventions},
	volume = {36},
	issn = {1468-0017},
	shorttitle = {Polysemy},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mila.12329},
	doi = {10.1111/mila.12329},
	abstract = {Polysemy, understood as instances of a single linguistic expression having multiple related senses, is not a homogenous phenomenon. There are regular (apparently, rule-based) cases and irregular (resemblance-based) cases, which have different processing profiles. Although a primary source of polysemy is pragmatic inference, at least some cases become conventionalised and linguistically encoded. Three main issues are discussed: (a) the key differences between regular and irregular cases and the role, if any, of a “core meaning”; (b) the distinction between pragmatic polysemy and semantic polysemy; and (c) the role of syntactic meaning in both generating and constraining polysemy.},
	language = {en},
	number = {1},
	urldate = {2025-03-15},
	journal = {Mind \& Language},
	author = {Carston, Robyn},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/mila.12329},
	keywords = {core meaning, homonymy, metonymy, polysemy, pragmatics, semantics},
	pages = {108--133},
}

@misc{noauthor_one_nodate,
	title = {One model for the learning of language},
	url = {https://www.pnas.org/doi/epub/10.1073/pnas.2021865119},
	language = {en},
	urldate = {2025-03-12},
	doi = {10.1073/pnas.2021865119},
}

@article{chomsky_opinion_2023,
	chapter = {Opinion},
	title = {Opinion {\textbar} {Noam} {Chomsky}: {The} {False} {Promise} of {ChatGPT}},
	issn = {0362-4331},
	shorttitle = {Opinion {\textbar} {Noam} {Chomsky}},
	url = {https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html},
	abstract = {The most prominent strain of A.I. encodes a flawed conception of language and knowledge.},
	language = {en-US},
	urldate = {2025-03-10},
	journal = {The New York Times},
	author = {Chomsky, Noam and Roberts, Ian and Watumull, Jeffrey},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, ChatGPT, Computers and the Internet, Ethics (Personal), Language and Languages, Philosophy},
}

@misc{noauthor_opinion_nodate,
	title = {Opinion - {Noam} {Chomsky} - {The} {False} {Promise} of {ChatGPT} - {The} {New} {York} {Times} {\textbar} {PDF} {\textbar} {Theory} {\textbar} {Artificial} {Intelligence}},
	url = {https://www.scribd.com/document/758830734/Opinion-Noam-Chomsky-The-False-Promise-of-ChatGPT-The-New-York-Times},
	abstract = {critica sobre AI},
	language = {en},
	urldate = {2025-03-08},
	journal = {Scribd},
}

@article{chomsky_opinion_2023-1,
	chapter = {Opinion},
	title = {Opinion {\textbar} {Noam} {Chomsky}: {The} {False} {Promise} of {ChatGPT}},
	issn = {0362-4331},
	shorttitle = {Opinion {\textbar} {Noam} {Chomsky}},
	url = {https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html},
	abstract = {The most prominent strain of A.I. encodes a flawed conception of language and knowledge.},
	language = {en-US},
	urldate = {2025-03-08},
	journal = {The New York Times},
	author = {Chomsky, Noam and Roberts, Ian and Watumull, Jeffrey},
	month = mar,
	year = {2023},
	keywords = {Artificial Intelligence, ChatGPT, Computers and the Internet, Ethics (Personal), Language and Languages, Philosophy},
}

@misc{piantadosi_modern_2024,
	title = {Modern language models refute {Chomsky}’s approach to language},
	url = {https://lingbuzz.net/lingbuzz/007180},
	abstract = {Modern machine learning has subverted and bypassed the theoretical framework of Chomsky’s generative approach to linguistics, including its core claims to particular insights, principles, structures, and processes. I describe the sense in which modern language models implement genuine theories of language, and I highlight the links between these models and approaches to linguistics that are based on gradient computations and memorized constructions. I also describe why these models undermine strong claims for the innateness of language and respond to several critiques of large language models, including arguments that they can’t answer “why” questions and skepticism that they are informative about real life acquisition. Most notably, large language models have attained remarkable success at discovering grammar without using any of the methods that some in linguistics insisted were necessary for a science of language to progress. (UPDATED: With a postscript on replies to the original draft)},
	urldate = {2025-03-08},
	publisher = {LingBuzz},
	author = {Piantadosi, Steven},
	month = jul,
	year = {2024},
	note = {LingBuzz Published In: Edward Gibson \& Moshe Poliak (eds.), From fieldwork to linguistic theory: A tribute to Dan Everett (Empirically Oriented Theoretical Morphology and Syntax 15), 353–414. Berlin: Language Science Press. https : //doi.org/10.5281/zenodo.12665933.},
	keywords = {chomsky, cognitive science, computational modeling, emergent, generative syntax, large language model, minimalism, statistical learning, syntax},
}

@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {https://aclanthology.org/N18-1202/},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2025-03-06},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {2227--2237},
}

@inproceedings{mccann_learned_2017,
	title = {Learned in {Translation}: {Contextualized} {Word} {Vectors}},
	volume = {30},
	shorttitle = {Learned in {Translation}},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/20c86a628232a67e7bd46f76fba7ce12-Abstract.html},
	abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
	urldate = {2025-03-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
	year = {2017},
}

@misc{shao_deepseekmath_2024,
	title = {{DeepSeekMath}: {Pushing} the {Limits} of {Mathematical} {Reasoning} in {Open} {Language} {Models}},
	shorttitle = {{DeepSeekMath}},
	url = {http://arxiv.org/abs/2402.03300},
	doi = {10.48550/arXiv.2402.03300},
	abstract = {Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7\% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9\% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.},
	urldate = {2025-03-04},
	publisher = {arXiv},
	author = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},
	month = apr,
	year = {2024},
	note = {arXiv:2402.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{dziri_faith_2023,
	title = {Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}},
	shorttitle = {Faith and {Fate}},
	url = {http://arxiv.org/abs/2305.18654},
	doi = {10.48550/arXiv.2305.18654},
	abstract = {Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with{\textbackslash},increased{\textbackslash},task{\textbackslash},complexity.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Welleck, Sean and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
	month = oct,
	year = {2023},
	note = {arXiv:2305.18654 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{stechly_gpt-4_2023,
	title = {{GPT}-4 {Doesn}'t {Know} {It}'s {Wrong}: {An} {Analysis} of {Iterative} {Prompting} for {Reasoning} {Problems}},
	shorttitle = {{GPT}-4 {Doesn}'t {Know} {It}'s {Wrong}},
	url = {http://arxiv.org/abs/2310.12397},
	doi = {10.48550/arXiv.2310.12397},
	abstract = {There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to the performance of iterative prompting. We show that the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Stechly, Kaya and Marquez, Matthew and Kambhampati, Subbarao},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12397 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{zhong_polysemy_2021,
	title = {Polysemy {Deciphering} {Network} for {Robust} {Human}–{Object} {Interaction} {Detection}},
	volume = {129},
	issn = {0920-5691, 1573-1405},
	url = {https://link.springer.com/10.1007/s11263-021-01458-8},
	doi = {10.1007/s11263-021-01458-8},
	language = {en},
	number = {6},
	urldate = {2025-03-03},
	journal = {International Journal of Computer Vision},
	author = {Zhong, Xubin and Ding, Changxing and Qu, Xian and Tao, Dacheng},
	month = jun,
	year = {2021},
	pages = {1910--1929},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-02-22},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{steinert-threlkeld_towards_2019,
	title = {Towards the {Emergence} of {Non}-trivial {Compositionality}},
	url = {https://philsci-archive.pitt.edu/16750/},
	urldate = {2025-02-12},
	author = {Steinert-Threlkeld, Shane},
	year = {2019},
}

@inproceedings{jasbi_parents_2021,
	title = {Parents’ and {Children}’s {Production} of {English} {Negation}},
	url = {https://www.semanticscholar.org/paper/Parents%E2%80%99-and-Children%E2%80%99s-Production-of-English-McDermott-Hinman-Davidson/58bb24750f17903e6bd7118fbccc7a3bc9dbf7f1},
	abstract = {Previous research has proposed several stages for children’s production of negative morphemes. For example, Cameron-Faulkner, Lieven, and Theakston (2007) proposed that English negative morphemes appear with a no{\textgreater}not{\textgreater}n’t order in children’s speech. Klima and Bellugi (1966) proposed that negation first appears outside the sentence and later moves inside between the subject and the verb. They also proposed that can’t and don’t are learned as unanalyzed wholes before their positive auxiliary variants. However, comprehension studies have not provided evidence for such stages yet (Austin et al. 2014; Feiman et al. 2017; Reuter, Feiman, and Snedeker 2018). This discrepancy can be explained in two ways. First, the lack of evidence may be due to limitations in comprehension studies. Second, the proposed stages may be limited to production and not generalizable to comprehension. This paper presents two exploratory corpus studies that support the second possibility. The results suggest that some previous stage hypotheses do not hold generally and may be limited to a few children. Furthermore, stages that do hold across children may be limited to production only. In the following section, we explain the previous stage hypotheses proposed for the development of negation. Section 3 presents our study on the relative frequency and emergence of no, not, and n’t in children’s speech. Section 4 presents our second study that uses part of speech tagging to examine Klima and Bellugi (1966)’s proposed stages. We summarize our findings and discuss future directions in Section 5.},
	urldate = {2025-02-12},
	author = {Jasbi, Masoud and McDermott-Hinman, Annika and Davidson, Kathryn and Carey, Susan},
	year = {2021},
}

@book{macwhinney_childes_2014,
	title = {The {CHILDES} project: {Tools} for analyzing talk, {Volume} {I}: {Transcription} format and programs},
	shorttitle = {The {CHILDES} project},
	url = {https://www.taylorfrancis.com/books/mono/10.4324/9781315805672/childes-project-brian-macwhinney},
	urldate = {2025-02-12},
	publisher = {Psychology Press},
	author = {MacWhinney, Brian},
	year = {2014},
}

@inproceedings{tullo_modelling_2003,
	title = {Modelling {Zipfian} distributions in language},
	url = {http://www.ling.ed.ac.uk/~jim/zipfjrh.pdf},
	urldate = {2025-02-12},
	booktitle = {Proceedings of language evolution and computation workshop/course at {ESSLLI}},
	author = {Tullo, Catriona and Hurford, James},
	year = {2003},
	pages = {62--75},
}

@misc{steinert-threlkeld_towards_2019-1,
	type = {Preprint},
	title = {Towards the {Emergence} of {Non}-trivial {Compositionality}},
	url = {https://philsci-archive.pitt.edu/16750/},
	abstract = {All natural languages exhibit a distinction between content words (nouns, verbs,etc.) and function words (determiners, auxiliaries, tenses, etc.). Yet surprisingly little has been said about the emergence of this universal architectural feature of human language. This paper argues that the existence of this distinction requires the presence of non-trivial compositionality and identifies assumptions that have previously been made in the literature that provably guarantee only trivial composition. It then presents a signaling game with variable contexts and shows how the distinction can emerge via reinforcement learning.},
	language = {en},
	urldate = {2025-02-12},
	journal = {Philosophy of Science},
	author = {Steinert-Threlkeld, Shane},
	month = dec,
	year = {2019},
	note = {Publisher: University of Chicago Press},
}

@article{feldman_how_2019,
	title = {How {Young} {Children} {Learn} {Language} and {Speech}},
	volume = {40},
	issn = {1526-3347},
	doi = {10.1542/pir.2017-0325},
	abstract = {Pediatric clinicians are on the front line for prevention of language and speech disorders. This review uses prevailing theories and recent data to justify strategies for prevention, screening and detection, diagnosis, and treatment of language and speech disorders. Primary prevention rests on theories that language learning is an interaction between the child's learning capacities and the language environment. Language learning occurs in a social context with active child engagement. Theories support parent education and public programs that increase children's exposure to child-directed speech. Early detection of delays requires knowledge of language milestones and recognition of high-risk indicators for disorders. Male sex, bilingual environments, birth order, and chronic otitis media are not adequate explanations for significant delays in language or speech. Current guidelines recommend both general and autism-specific screening. Environmental and genetic factors contribute to primary language and speech disorders. Secondary and tertiary prevention requires early identification of children with language and speech disorders. Disorders may be found in association with chromosomal, genetic, neurologic, and other health conditions. Systematic reviews find that speech-language therapy, alone or in conjunction with other developmental services, is effective for many disorders. Speech-language interventions alter the environment and stimulate children's targeted responding to improve their skills.},
	language = {eng},
	number = {8},
	journal = {Pediatrics in Review},
	author = {Feldman, Heidi M.},
	month = aug,
	year = {2019},
	pmid = {31371633},
	pmcid = {PMC7236655},
	keywords = {Child, Child, Preschool, Humans, Language Development, Language Disorders, Preventive Health Services, Speech},
	pages = {398--411},
}

@book{clark_first_2009,
	address = {Cambridge},
	edition = {2},
	title = {First {Language} {Acquisition}},
	url = {https://www.cambridge.org/core/books/first-language-acquisition/DFE9CE841BD657FFF2639065E912E738},
	abstract = {Babies are not born talking, they learn language, starting immediately from birth. How does this process take place? When do children master the skills needed for using language successfully? What stages do they go through as they learn to understand and talk? Do the languages they learn affect the way they think? This edition of Eve Clark's highly successful textbook focuses on children's acquisition of a first language, the stages of development they go through, and how they use language as they learn. It reports on recent findings in each area covered, includes a completely new chapter on the acquisition of two languages and shows how speech to children differs by social class. Skilfully integrating actual data with coverage of current theories and debates, it is an essential guide to studying language acquisition for those working in linguistics, developmental psychology and cognitive science.},
	urldate = {2025-02-12},
	publisher = {Cambridge University Press},
	author = {Clark, Eve V.},
	year = {2009},
	doi = {10.1017/CBO9780511806698},
}

@article{choi_infants_2018,
	title = {Infants' understanding of the definite/indefinite article in a third-party communicative situation},
	volume = {175},
	issn = {1873-7838},
	doi = {10.1016/j.cognition.2018.02.006},
	abstract = {The present study examines how infants use their emergent perspective-taking and language comprehension abilities to make sense of interactions between two human agents. In the study, one agent (Agent1) could see only one of two identical balls on an apparatus because of a screen obstructing her view while the infant and another agent (Agent2) could see both balls. 19-month-old English-learning monolingual infants seemed to expect Agent2 to grasp the ball visible to Agent1 when she said to Agent2 "Give me the ball" but not when she said "Give me a ball." 14-month-olds appeared to accept that Agent2 could grasp either ball when Agent1 said "Give me the ball." Therefore, by 19 months of age, English-learning infants seem to attend to the specific linguistic units used, e.g., the definite article, to identify the referent of others' speech. Possible reasons in connection with language acquisition processes and/or environmental factors for the two age groups' respective failures with the definite and the indefinite articles are discussed.},
	language = {eng},
	journal = {Cognition},
	author = {Choi, You-Jung and Song, Hyun-Joo and Luo, Yuyan},
	month = jun,
	year = {2018},
	pmid = {29475192},
	keywords = {Child Development, Communication, Comprehension, Female, Humans, Infant, Infant cognition, Language Development, Language comprehension, Male, Perspective-taking, Social Behavior, Social interactions},
	pages = {69--76},
}

@book{nemeth_pragmatics_2022,
	title = {Pragmatics and the {Flexibility} of {Word} {Meaning}},
	isbn = {978-0-585-47426-7},
	abstract = {Recently, the investigation of word meaning in utterances has connected two different fields: lexical semantics and pragmatics. A new linguistic discipline, namely lexical pragmatics, is emerging. The eleven papers of the present book constitute a unit in the sense that they have a common aim: to explore the interaction between lexical semantics and pragmatics. The authors examine phenomena such as productive sense extension, regular polysemy, multifunctionality, implicit arguments and predicates, and non-typical anaphoric pronouns, on the basis of linguistic data, for instance, from English, Norwegian, Russian, and Hungarian, as well as using a great variety of frameworks (optimality framework, two-level semantics, the theory of generative lexicon, cognitive grammar, Gricean theory, and relevance theory.},
	language = {en},
	publisher = {BRILL},
	author = {Nemeth, Eniko and Bibok, Károly},
	month = feb,
	year = {2022},
	note = {Google-Books-ID: 1\_VgEAAAQBAJ},
	keywords = {Language Arts \& Disciplines / Linguistics / Etymology, Language Arts \& Disciplines / Linguistics / General, Language Arts \& Disciplines / Linguistics / Historical \& Comparative, Language Arts \& Disciplines / Linguistics / Semantics},
}

@article{srinivasan_children_2019,
	title = {Children use polysemy to structure new word meanings},
	volume = {148},
	issn = {1939-2222},
	doi = {10.1037/xge0000454},
	abstract = {It is well-known that children rapidly learn words, following a range of heuristics. What is less well appreciated is that—because most words are polysemous and have multiple meanings (e.g., “glass” can label a material and drinking vessel)—children will often be learning a new meaning for a known word, rather than an entirely new word. Across 4 experiments we show that children flexibly adapt a well-known heuristic—the shape bias—when learning polysemous words. Consistent with previous studies, we find that children and adults preferentially extend a new object label to other objects of the same shape. But we also find that when a new word for an object (“a gup”) has previously been used to label the material composing that object (“some gup”), children and adults override the shape bias, and are more likely to extend the object label by material (Experiments 1 and 3). Further, we find that, just as an older meaning of a polysemous word constrains interpretations of a new word meaning, encountering a new word meaning leads learners to update their interpretations of an older meaning (Experiment 2). Finally, we find that these effects only arise when learners can perceive that a word’s meanings are related, not when they are arbitrarily paired (Experiment 4). Together, these findings show that children can exploit cues from polysemy to infer how new word meanings should be extended, suggesting that polysemy may facilitate word learning and invite children to construe categories in new ways. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
	number = {5},
	journal = {Journal of Experimental Psychology: General},
	author = {Srinivasan, Mahesh and Berner, Catherine and Rabagliati, Hugh},
	year = {2019},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Childhood Development, Form and Shape Perception, Learning, Test Construction, Vocabulary, Word Meaning},
	pages = {926--942},
}

@misc{grindrod_transformers_2024,
	title = {Transformers, {Contextualism}, and {Polysemy}},
	url = {http://arxiv.org/abs/2404.09577},
	doi = {10.48550/arXiv.2404.09577},
	abstract = {The transformer architecture, introduced by Vaswani et al. (2017), is at the heart of the remarkable recent progress in the development of language models, including widely-used chatbots such as Chat-GPT and Claude. In this paper, I argue that we can extract from the way the transformer architecture works a theory of the relationship between context and meaning. I call this the transformer theory, and I argue that it is novel with regard to two related philosophical debates: the contextualism debate regarding the extent of context-sensitivity across natural language, and the polysemy debate regarding how polysemy should be captured within an account of word meaning.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Grindrod, Jumbly},
	month = sep,
	year = {2024},
	note = {arXiv:2404.09577 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{setitra_leveraging_2025,
	title = {Leveraging ensemble deep models and llm for visual polysemy and word sense disambiguation},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-024-20235-6},
	doi = {10.1007/s11042-024-20235-6},
	abstract = {Visual Polysemy Disambiguation (VPD) and Visual Word Sense Disambiguation (VWSD) are challenging tasks for both computer vision and NLP since an image can have diverse contextual interpretations, ranging from visual representations to abstract concepts. In this paper, we propose a novel approach to address the challenges of VPD and VWSD by leveraging ensemble deep models from computer vision to alleviate the problem of VPD and using the strength of LLMs to mitigate the problem of WSD. We first generate visually representative images from textual descriptions through a zero-shot text-to-image generation framework using image scrapping and Google search. We then employ an ensemble of classifiers and a deep network to learn feature representations, classify images into contexts and find the best match. Similarly, we perform the reverse process of generating textual descriptions from images using a Vision Transformer model and calculate the cosine distance with the actual text. Experimental evaluation of benchmark datasets demonstrates the effectiveness of our combined approach in strengthening both text-to-image and image-to-text generation, we improve disambiguation accuracy, providing a robust solution for VWSD with an MRR of \$\$95.77{\textbackslash}\%\$\$and a Hit rate of \$\$92.00{\textbackslash}\%\$\$surpassing state-of-the-art methods.},
	language = {en},
	urldate = {2025-02-12},
	journal = {Multimedia Tools and Applications},
	author = {Setitra, Insaf and Rajapaksha, Praboda and Myat, Aung Kaung and Crespi, Noel},
	month = jan,
	year = {2025},
	keywords = {Artificial Intelligence, Classification, Multimodal representation, Text-to-image generation, ViT, Visual polysemy, Visual word sense disambiguation},
}

@misc{houlsby_parameter-efficient_2019,
	title = {Parameter-{Efficient} {Transfer} {Learning} for {NLP}},
	url = {http://arxiv.org/abs/1902.00751},
	doi = {10.48550/arXiv.1902.00751},
	abstract = {Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4\% of the performance of full fine-tuning, adding only 3.6\% parameters per task. By contrast, fine-tuning trains 100\% of the parameters per task.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin de and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	month = jun,
	year = {2019},
	note = {arXiv:1902.00751 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{yu_exploring_2023,
	title = {Exploring the {Residual} {Stream} of {Transformers}},
	url = {http://arxiv.org/abs/2312.12141},
	doi = {10.48550/arXiv.2312.12141},
	abstract = {Transformer-based models have achieved great breakthroughs in recent years. However, there are many significant questions that have not been answered in the field of explaining the reason why the models have powerful outputs. We do not know how to locate the models' important parameters storing the knowledge for predicting the next word, and whether these parameters are stored on the same layer/module or different ones. Moreover, we do not understand the mechanism to merge the knowledge into the final embedding for next word prediction. In this paper, we explore the residual stream of transformers to increase the interpretability. We find the mechanism behind residual connection is a direct addition function on before-softmax values, so the probabilities of tokens with larger before-softmax values will increase. Moreover, we prove that using log probability increase as contribution scores is reasonable, and based on this we can locate important parameters. Besides, we propose a method to analyze how previous layers affect upper layers by comparing the inner products. The experimental results and case study show that our research can increase the interpretability of transformer-based models. We will release our code on https://github.com/zepingyu0512/residualstream.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Yu, Zeping and Yang, Kailai and Liu, Zhiwei and Ananiadou, Sophia},
	month = dec,
	year = {2023},
	note = {arXiv:2312.12141 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_exploring_nodate,
	title = {Exploring the {Residual} {Stream} of {Transformers}},
	url = {https://arxiv.org/html/2312.12141v1/#bib.bib4},
	urldate = {2025-01-31},
}

@misc{cao_editing_2021,
	title = {Editing {Factual} {Knowledge} in {Language} {Models}},
	url = {http://arxiv.org/abs/2104.08164},
	doi = {10.48550/arXiv.2104.08164},
	abstract = {The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix 'bugs' or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor's efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a 'probe' revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Cao, Nicola De and Aziz, Wilker and Titov, Ivan},
	month = sep,
	year = {2021},
	note = {arXiv:2104.08164 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kozma_theoretical_2024,
	title = {Theoretical {Analysis} of {Byte}-{Pair} {Encoding}},
	url = {http://arxiv.org/abs/2411.08671},
	doi = {10.48550/arXiv.2411.08671},
	abstract = {Byte-Pair Encoding (BPE) is a widely used method for subword tokenization, with origins in grammar-based text compression. It is employed in a variety of language processing tasks such as machine translation or large language model (LLM) pretraining, to create a token dictionary of a prescribed size. Most evaluations of BPE to date are empirical, and the reasons for its good practical performance are not well understood. In this paper we focus on the optimization problem underlying BPE: finding a pair encoding that achieves optimal compression utility. We show that this problem is APX-complete, indicating that it is unlikely to admit a polynomial-time approximation scheme. This answers, in a stronger form, a question recently raised by Zouhar et al. On the positive side, we show that BPE approximates the compression utility of the optimal pair encoding to a worst-case factor between \$0.333\$ and \$0.625\$. Our results aim to explain the ongoing success of BPE and are, to our knowledge, the first rigorous guarantees on its compression utility that hold for all inputs.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Kozma, László and Voderholzer, Johannes},
	month = nov,
	year = {2024},
	note = {arXiv:2411.08671 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Data Structures and Algorithms},
}

@misc{hu_lora_2021-1,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_mixture_nodate,
	title = {Mixture of {Experts} {Explained}},
	url = {https://huggingface.co/blog/moe},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2025-01-28},
}

@misc{deepseek-ai_deepseek-r1_2025-1,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{zhang_dont_2023,
	address = {Singapore},
	title = {Don`t {Trust} {ChatGPT} when your {Question} is not in {English}: {A} {Study} of {Multilingual} {Abilities} and {Types} of {LLMs}},
	shorttitle = {Don`t {Trust} {ChatGPT} when your {Question} is not in {English}},
	url = {https://aclanthology.org/2023.emnlp-main.491/},
	doi = {10.18653/v1/2023.emnlp-main.491},
	abstract = {Large language models (LLMs) have demonstrated exceptional natural language understanding abilities, and have excelled in a variety of natural language processing (NLP) tasks. Despite the fact that most LLMs are trained predominantly on English, multiple studies have demonstrated their capabilities in a variety of languages. However, fundamental questions persist regarding how LLMs acquire their multilingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing how they use LLMs and interpret their output. In this work, we propose a systematic way of qualitatively and quantitatively evaluating the multilingual capabilities of LLMs. We investigate the phenomenon of cross-language generalization in LLMs, wherein limited multilingual training data leads to advanced multilingual capabilities. To accomplish this, we employ a novel prompt back-translation method. The results demonstrate that LLMs, such as GPT, can effectively transfer learned knowledge across different languages, yielding relatively consistent results in translation-equivariant tasks, in which the correct output does not depend on the language of the input. However, LLMs struggle to provide accurate results in translation-variant tasks, which lack this property, requiring careful user judgment to evaluate the answers.},
	urldate = {2025-01-28},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Xiang and Li, Senyu and Hauer, Bradley and Shi, Ning and Kondrak, Grzegorz},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {7915--7927},
}

@misc{sennrich_neural_2016,
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://arxiv.org/abs/1508.07909},
	doi = {10.48550/arXiv.1508.07909},
	abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
	urldate = {2025-01-24},
	publisher = {arXiv},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = jun,
	year = {2016},
	note = {arXiv:1508.07909 [cs]
version: 5},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{papadimitriou_multilingual_2023,
	address = {Dubrovnik, Croatia},
	title = {Multilingual {BERT} has an accent: {Evaluating} {English} influences on fluency in multilingual models},
	shorttitle = {Multilingual {BERT} has an accent},
	url = {https://aclanthology.org/2023.findings-eacl.89/},
	doi = {10.18653/v1/2023.findings-eacl.89},
	abstract = {While multilingual language models can improve NLP performance on low-resource languages by leveraging higher-resource languages, they also reduce average performance on all languages (the ‘curse of multilinguality'). Here we show another problem with multilingual models: grammatical structures in higher-resource languages bleed into lower-resource languages, a phenomenon we call grammatical structure bias. We show this bias via a novel method for comparing the fluency of multilingual models to the fluency of monolingual Spanish and Greek models: testing their preference for two carefully-chosen variable grammatical structures (optional pronoun-drop in Spanish and optional Subject-Verb ordering in Greek). We find that multilingual BERT is biased toward the English-like setting (explicit pronouns and Subject-Verb-Object ordering) as compared to our monolingual control language model. With our case studies, we hope to bring to light the fine-grained ways in which multilingual models can be biased, and encourage more linguistically-aware fluency evaluation.},
	urldate = {2025-01-22},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Papadimitriou, Isabel and Lopez, Kezia and Jurafsky, Dan},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {1194--1200},
}

@inproceedings{wu_are_2020,
	address = {Online},
	title = {Are {All} {Languages} {Created} {Equal} in {Multilingual} {BERT}?},
	url = {https://aclanthology.org/2020.repl4nlp-1.16/},
	doi = {10.18653/v1/2020.repl4nlp-1.16},
	abstract = {Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.},
	urldate = {2025-01-22},
	booktitle = {Proceedings of the 5th {Workshop} on {Representation} {Learning} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Shijie and Dredze, Mark},
	editor = {Gella, Spandana and Welbl, Johannes and Rei, Marek and Petroni, Fabio and Lewis, Patrick and Strubell, Emma and Seo, Minjoon and Hajishirzi, Hannaneh},
	month = jul,
	year = {2020},
	pages = {120--130},
}

@inproceedings{wu_are_2020-1,
	address = {Online},
	title = {Are {All} {Languages} {Created} {Equal} in {Multilingual} {BERT}?},
	url = {https://aclanthology.org/2020.repl4nlp-1.16/},
	doi = {10.18653/v1/2020.repl4nlp-1.16},
	abstract = {Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.},
	urldate = {2025-01-22},
	booktitle = {Proceedings of the 5th {Workshop} on {Representation} {Learning} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Shijie and Dredze, Mark},
	editor = {Gella, Spandana and Welbl, Johannes and Rei, Marek and Petroni, Fabio and Lewis, Patrick and Strubell, Emma and Seo, Minjoon and Hajishirzi, Hannaneh},
	month = jul,
	year = {2020},
	pages = {120--130},
}

@inproceedings{wu_are_2020-2,
	address = {Online},
	title = {Are {All} {Languages} {Created} {Equal} in {Multilingual} {BERT}?},
	url = {https://aclanthology.org/2020.repl4nlp-1.16/},
	doi = {10.18653/v1/2020.repl4nlp-1.16},
	abstract = {Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.},
	urldate = {2025-01-22},
	booktitle = {Proceedings of the 5th {Workshop} on {Representation} {Learning} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Shijie and Dredze, Mark},
	editor = {Gella, Spandana and Welbl, Johannes and Rei, Marek and Petroni, Fabio and Lewis, Patrick and Strubell, Emma and Seo, Minjoon and Hajishirzi, Hannaneh},
	month = jul,
	year = {2020},
	pages = {120--130},
}

@misc{zhou_least--most_2023,
	title = {Least-to-{Most} {Prompting} {Enables} {Complex} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2205.10625},
	doi = {10.48550/arXiv.2205.10625},
	abstract = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Zhou, Denny and Schärli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
	month = apr,
	year = {2023},
	note = {arXiv:2205.10625 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{drozdov_compositional_2022,
	title = {Compositional {Semantic} {Parsing} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2209.15003},
	doi = {10.48550/arXiv.2209.15003},
	abstract = {Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1\% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Drozdov, Andrew and Schärli, Nathanael and Akyürek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
	month = sep,
	year = {2022},
	note = {arXiv:2209.15003 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{khot_decomposed_2023,
	title = {Decomposed {Prompting}: {A} {Modular} {Approach} for {Solving} {Complex} {Tasks}},
	shorttitle = {Decomposed {Prompting}},
	url = {http://arxiv.org/abs/2210.02406},
	doi = {10.48550/arXiv.2210.02406},
	abstract = {Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
	month = apr,
	year = {2023},
	note = {arXiv:2210.02406 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{dua_successive_2022,
	title = {Successive {Prompting} for {Decomposing} {Complex} {Questions}},
	url = {http://arxiv.org/abs/2212.04092},
	doi = {10.48550/arXiv.2212.04092},
	abstract = {Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce ``Successive Prompting'', where we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate a synthetic dataset which can be used to bootstrap a model's ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement of {\textasciitilde}5\% absolute F1 on a few-shot version of the DROP dataset when compared with a state-of-the-art model with the same supervision.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Dua, Dheeru and Gupta, Shivanshu and Singh, Sameer and Gardner, Matt},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04092 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{weng_large_2023,
	title = {Large {Language} {Models} are {Better} {Reasoners} with {Self}-{Verification}},
	url = {http://arxiv.org/abs/2212.09561},
	doi = {10.48550/arXiv.2212.09561},
	abstract = {Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Weng, Yixuan and Zhu, Minjun and Xia, Fei and Li, Bin and He, Shizhu and Liu, Shengping and Sun, Bin and Liu, Kang and Zhao, Jun},
	month = oct,
	year = {2023},
	note = {arXiv:2212.09561 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{li_making_2023,
	title = {Making {Large} {Language} {Models} {Better} {Reasoners} with {Step}-{Aware} {Verifier}},
	url = {http://arxiv.org/abs/2206.02336},
	doi = {10.48550/arXiv.2206.02336},
	abstract = {Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9\% to 58.1\% in problem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DIVERSE has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DIVERSE on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4\% to 83.2\%).},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
	month = may,
	year = {2023},
	note = {arXiv:2206.02336 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{cobbe_training_2021,
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {http://arxiv.org/abs/2110.14168},
	doi = {10.48550/arXiv.2110.14168},
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	month = nov,
	year = {2021},
	note = {arXiv:2110.14168 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ye_unreliability_2022,
	title = {The {Unreliability} of {Explanations} in {Few}-shot {Prompting} for {Textual} {Reasoning}},
	url = {http://arxiv.org/abs/2205.03401},
	doi = {10.48550/arXiv.2205.03401},
	abstract = {Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Ye, Xi and Durrett, Greg},
	month = oct,
	year = {2022},
	note = {arXiv:2205.03401 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{fu_complexity-based_2023,
	title = {Complexity-{Based} {Prompting} for {Multi}-{Step} {Reasoning}},
	url = {http://arxiv.org/abs/2210.00720},
	doi = {10.48550/arXiv.2210.00720},
	abstract = {We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
	month = jan,
	year = {2023},
	note = {arXiv:2210.00720 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{huang_towards_2023,
	address = {Toronto, Canada},
	title = {Towards {Reasoning} in {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Towards {Reasoning} in {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.findings-acl.67/},
	doi = {10.18653/v1/2023.findings-acl.67},
	abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
	urldate = {2025-01-14},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Jie and Chang, Kevin Chen-Chuan},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {1049--1065},
}

@misc{martinez_evaluating_2024,
	title = {Evaluating the {Existence} {Proof}: {LLMs} as {Cognitive} {Models} of {Language} {Acquisition}},
	shorttitle = {Evaluating the {Existence} {Proof}},
	url = {https://lingbuzz.net/lingbuzz/008277},
	abstract = {In recent years, the technological success of large language models (LLMs) has been taken as an existence proof that language acquisition may succeed without  domain-specific principles and constraints. While this argument acknowledges the important differences between LLM training and child language acquisition, its validity rests on the validity of the existence proof itself, that LLMs indeed demonstrate capacity comparable to human linguistic knowledge, the terminal state of the acquisition process. We contend that such a proof has not been delivered, in large part due to the lack of rigor in LLM evaluation and the absence of serious engagement with the empirical study of child language. When trained on child-scale input data and evaluated on widely used benchmarks, LLMs can be readily matched by simple baseline models that are demonstrably inadequate for human language. As a partial remedy, we advocate for the use of thoroughly validated datasets that more accurately reflect the scope of linguistic knowledge. On these datasets, even LLMs trained on very large amounts of data perform  in a way inconsistent with human behavior. The burden of an existence proof is considerably heavier than previously realized.},
	urldate = {2024-12-23},
	publisher = {LingBuzz},
	author = {Martinez, Hector Javier Vazquez and Heuser, Annika and Yang, Charles and Kodner, Jordan},
	month = jul,
	year = {2024},
	note = {LingBuzz Published In: Forthcoming. In Artificial Knowledge of Language. José-Luis Mendívil-Giró, editor. Vernon Press},
	keywords = {benchmarking, cognitive modeling, computational linguistics, evaluation, language acquisition, llm, llms, methodology, neural networks, syntax},
}

@article{morse_why_2017,
	title = {Why {Are} {There} {Developmental} {Stages} in {Language} {Learning}? {A} {Developmental} {Robotics} {Model} of {Language} {Development}},
	volume = {41},
	copyright = {Copyright © 2016 Cognitive Science Society, Inc.},
	issn = {1551-6709},
	shorttitle = {Why {Are} {There} {Developmental} {Stages} in {Language} {Learning}?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12390},
	doi = {10.1111/cogs.12390},
	abstract = {Most theories of learning would predict a gradual acquisition and refinement of skills as learning progresses, and while some highlight exponential growth, this fails to explain why natural cognitive development typically progresses in stages. Models that do span multiple developmental stages typically have parameters to “switch” between stages. We argue that by taking an embodied view, the interaction between learning mechanisms, the resulting behavior of the agent, and the opportunities for learning that the environment provides can account for the stage-wise development of cognitive abilities. We summarize work relevant to this hypothesis and suggest two simple mechanisms that account for some developmental transitions: neural readiness focuses on changes in the neural substrate resulting from ongoing learning, and perceptual readiness focuses on the perceptual requirements for learning new tasks. Previous work has demonstrated these mechanisms in replications of a wide variety of infant language experiments, spanning multiple developmental stages. Here we piece this work together as a single model of ongoing learning with no parameter changes at all. The model, an instance of the Epigenetic Robotics Architecture (Morse et al 2010) embodied on the iCub humanoid robot, exhibits ongoing multi-stage development while learning pre-linguistic and then basic language skills.},
	language = {en},
	number = {S1},
	urldate = {2024-12-11},
	journal = {Cognitive Science},
	author = {Morse, Anthony F. and Cangelosi, Angelo},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12390},
	keywords = {Cognitive architecture, Developmental robotics, Language learning, Ongoing development},
	pages = {32--51},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	language = {en},
	number = {7587},
	urldate = {2024-12-20},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Reward},
	pages = {484--489},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-12-20},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{brian_childes_2000,
	title = {The {CHILDES} {Project}: {Tools} for analyzing talk},
	shorttitle = {The {CHILDES} {Project}},
	journal = {Mahwah, NJ \& London: Lawrence Erlbaum},
	author = {Brian, Macwhinney},
	year = {2000},
}

@article{brian_childes_2000-1,
	title = {The {CHILDES} {Project}: {Tools} for analyzing talk},
	shorttitle = {The {CHILDES} {Project}},
	journal = {Mahwah, NJ \& London: Lawrence Erlbaum},
	author = {Brian, Macwhinney},
	year = {2000},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} prompting elicits reasoning in large language models},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-11-04},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_learning_2024,
	title = {Learning to reason with {LLMs}},
	url = {https://openai.com/index/learning-to-reason-with-llms/},
	abstract = {We are introducing OpenAI o1, a new large language model trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers—it can produce a long internal chain of thought before responding to the user.},
	language = {en-US},
	urldate = {2024-11-18},
	month = sep,
	year = {2024},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
}

@phdthesis{jasbi_learning_2018,
	address = {United States -- California},
	type = {Ph.{D}.},
	title = {Learning {Disjunction}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2435868005/abstract/71DF40E726D64C06PQ/1},
	abstract = {To understand language, we rely on mental representations of words and their meanings. What constitutes these representations? How are they learned? To address these questions, I investigate how children learn and interpret the disjunction word "or". The highly abstract and context-dependent interpretation of or challenges word learning theories and provides an exceptional opportunity to better understand how words are associated with their meanings.
"Or" has several interpretations, including exclusive and inclusive disjunction. Inclusive disjunction holds when A is true, B is true, or both. For example, a waiter may ask if you would like something to eat or drink, not excluding the possibility that you choose both. Exclusive disjunction is true when only A is true, or only B is true, but not both. If the waiter later asks whether you would like to see the dessert menu or have the check, his "or" is most likely interpreted as exclusive. He is suggesting that you should choose one or the other. Given these complexities in the interpretation of "or", how do children learn it?
A previous study has shown that when parents talk to their children, the majority of "or"-examples they use are exclusive. I present an annotation study on parents' speech to children that replicated this finding. Nevertheless, comprehension studies have found that preschool children understand the inclusive interpretation of disjunction around four years of age. In an experimental study with a novel paradigm, I replicated this finding in simple existential sentences. These two findings lead to a puzzle in the literature: How can children learn the inclusive interpretation of "or" if they rarely hear it?
I argue that this puzzle arises in models of word learning that directly map words to their meanings, thereby ignoring accompanying linguistic and conceptual cues. I present an in-depth annotation study demonstrating that exclusive interpretations correlate with contextual cues (such as intonation and the semantic relation of the alternatives "or" combines with) in children's input. Applying supervised learning techniques to the annotated data, I found that a learner who makes use of these contextual cues can learn the inclusive as well as exclusive interpretation of disjunction from the language heard.
These findings indicate that the representation of a word like "or" cannot be isolated from the linguistic and conceptual environment in which it appears. The linguistic and conceptual aspects of "or"'s environment can act as cues that aid its acquisition and interpretation. Together, these studies show that learning a function word like "or" requires richer lexical representations than are currently assumed by our theories of word learning.},
	language = {English},
	urldate = {2024-12-17},
	school = {Stanford University},
	author = {Jasbi, Masoud},
	year = {2018},
	note = {ISBN: 9798662556959},
	keywords = {Linguistics, Word learning},
}

@misc{baroni_proper_2022,
	title = {On the proper role of linguistically-oriented deep net analysis in linguistic theorizing},
	url = {http://arxiv.org/abs/2106.08694},
	doi = {10.48550/arXiv.2106.08694},
	abstract = {A lively research field has recently emerged that uses experimental methods to probe the linguistic behavior of modern deep networks. While work in this tradition often reports intriguing results about the grammatical skills of deep nets, it is not clear what their implications for linguistic theorizing should be. As a consequence, linguistically-oriented deep net analysis has had very little impact on linguistics at large. In this chapter, I suggest that deep networks should be treated as theories making explicit predictions about the acceptability of linguistic utterances. I argue that, if we overcome some obstacles standing in the way of seriously pursuing this idea, we will gain a powerful new theoretical tool, complementary to mainstream algebraic approaches.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Baroni, Marco},
	month = mar,
	year = {2022},
	note = {arXiv:2106.08694 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{hao_training_2024,
	title = {Training {Large} {Language} {Models} to {Reason} in a {Continuous} {Latent} {Space}},
	url = {http://arxiv.org/abs/2412.06769},
	doi = {10.48550/arXiv.2412.06769},
	abstract = {Large language models (LLMs) are restricted to reason in the "language space", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed "continuous thought"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.},
	urldate = {2024-12-16},
	publisher = {arXiv},
	author = {Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong},
	month = dec,
	year = {2024},
	note = {arXiv:2412.06769 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{piantadosi_modern_2024-1,
	title = {Modern language models refute {Chomsky}’s approach to language},
	url = {https://lingbuzz.net/lingbuzz/007180},
	abstract = {Modern machine learning has subverted and bypassed the theoretical framework of Chomsky’s generative approach to linguistics, including its core claims to particular insights, principles, structures, and processes. I describe the sense in which modern language models implement genuine theories of language, and I highlight the links between these models and approaches to linguistics that are based on gradient computations and memorized constructions. I also describe why these models undermine strong claims for the innateness of language and respond to several critiques of large language models, including arguments that they can’t answer “why” questions and skepticism that they are informative about real life acquisition. Most notably, large language models have attained remarkable success at discovering grammar without using any of the methods that some in linguistics insisted were necessary for a science of language to progress. (UPDATED: With a postscript on replies to the original draft)},
	urldate = {2024-12-13},
	publisher = {LingBuzz},
	author = {Piantadosi, Steven},
	month = jul,
	year = {2024},
	note = {LingBuzz Published In: Edward Gibson \& Moshe Poliak (eds.), From fieldwork to linguistic theory: A tribute to Dan Everett (Empirically Oriented Theoretical Morphology and Syntax 15), 353–414. Berlin: Language Science Press. https : //doi.org/10.5281/zenodo.12665933.},
	keywords = {chomsky, cognitive science, computational modeling, emergent, generative syntax, large language model, minimalism, statistical learning, syntax},
}

@article{jasbi_adults_2021,
	title = {Adults’ and {Children}’s {Comprehension} of {Linguistic} {Disjunction}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by/4.0},
	issn = {2474-7394},
	url = {https://online.ucpress.edu/collabra/article/7/1/27702/118659/Adults-and-Children-s-Comprehension-of-Linguistic},
	doi = {10.1525/collabra.27702},
	abstract = {Disjunction has played a major role in advancing theories of logic, language, and cognition, featuring as the centerpiece of debates on the origins and development of logical thought. Recent studies have argued that due to non-adult-like pragmatic reasoning, preschool children’s comprehension of linguistic disjunction differs from adults in two ways. First, children are more likely to interpret “or” as “and” (conjunctive interpretations); Second, children are more likely to consider a disjunction as inclusive (lack of exclusivity implicatures). We tested adults and children’s comprehension of disjunction in existential sentences using two and three-alternative forced choice tasks, and analyzed children’s spontaneous verbal reactions prior to their forced-choice judgments. Overall our results are compatible with studies that suggest children understand the basic truth-conditional semantics of disjunction. Children did not interpret “or” as “and”, supporting studies that argue conjunctive interpretations are due to task demands. In addition, even though our forced-choice tasks suggest children interpreted disjunction as inclusive, spontaneous verbal reactions showed that children were sensitive to the adult-like pragmatics of disjunction. Theoretically, these studies provide evidence against previous developmental accounts, and lend themselves to two alternative hypotheses. First, that preschool children’s pragmatic knowledge is more adult-like than previously assumed, but forced-choice judgments are not sensitive enough to capture this knowledge. Second, children may have the knowledge of the relevant lexical scale themselves, but be uncertain whether a new speaker also has this knowledge (mutual knowledge of the scale).},
	language = {en},
	number = {1},
	urldate = {2024-12-13},
	journal = {Collabra: Psychology},
	author = {Jasbi, Masoud and Frank, Michael C.},
	editor = {Vazire, Simine},
	month = sep,
	year = {2021},
	pages = {27702},
}

@article{piaget_origins_nodate,
	title = {{THE} {ORIGINS} {OF} {INTELLIGENCE} {IN} {CHILDREN}},
	language = {en},
	author = {Piaget, Jean},
}

@misc{stanford_medicine_your_2024,
	title = {Your {Child}'s {Growth} and {Development}},
	url = {https://www.stanfordchildrens.org/en/topic/default?id=your-childs-growth-and-development-85-P01019},
	abstract = {Detailed information on growth and development of children},
	language = {en-us},
	urldate = {2024-12-11},
	author = {{Stanford Medicine}},
	year = {2024},
}

@misc{baroni_proper_2022-1,
	title = {On the proper role of linguistically-oriented deep net analysis in linguistic theorizing},
	url = {http://arxiv.org/abs/2106.08694},
	doi = {10.48550/arXiv.2106.08694},
	abstract = {A lively research field has recently emerged that uses experimental methods to probe the linguistic behavior of modern deep networks. While work in this tradition often reports intriguing results about the grammatical skills of deep nets, it is not clear what their implications for linguistic theorizing should be. As a consequence, linguistically-oriented deep net analysis has had very little impact on linguistics at large. In this chapter, I suggest that deep networks should be treated as theories making explicit predictions about the acceptability of linguistic utterances. I argue that, if we overcome some obstacles standing in the way of seriously pursuing this idea, we will gain a powerful new theoretical tool, complementary to mainstream algebraic approaches.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Baroni, Marco},
	month = mar,
	year = {2022},
	note = {arXiv:2106.08694 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{wilcox_targeted_2021,
	address = {Online},
	title = {A {Targeted} {Assessment} of {Incremental} {Processing} in {Neural} {Language} {Models} and {Humans}},
	url = {https://aclanthology.org/2021.acl-long.76},
	doi = {10.18653/v1/2021.acl-long.76},
	abstract = {We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena. Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task. We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes. We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model `accuracy' scores a la Marvin and Linzen (2018) about equal. However, although language model outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences. Specifically, when models encounter syntactic violations they fail to accurately predict the longer reading times observed in the human data. These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations.},
	urldate = {2024-12-12},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wilcox, Ethan and Vani, Pranali and Levy, Roger},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {939--952},
}

@inproceedings{gulordava_colorless_2018,
	address = {New Orleans, Louisiana},
	title = {Colorless {Green} {Recurrent} {Networks} {Dream} {Hierarchically}},
	url = {https://aclanthology.org/N18-1108},
	doi = {10.18653/v1/N18-1108},
	abstract = {Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.},
	urldate = {2024-12-12},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {1195--1205},
}

@article{manning_emergent_2020,
	title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.1907367117},
	doi = {10.1073/pnas.1907367117},
	abstract = {This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.},
	number = {48},
	urldate = {2024-12-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
	month = dec,
	year = {2020},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {30046--30054},
}

@misc{piantadosi_modern_2024,
	title = {Modern language models refute {Chomsky}’s approach to language},
	url = {https://lingbuzz.net/lingbuzz/007180},
	abstract = {Modern machine learning has subverted and bypassed the theoretical framework of Chomsky’s generative approach to linguistics, including its core claims to particular insights, principles, structures, and processes. I describe the sense in which modern language models implement genuine theories of language, and I highlight the links between these models and approaches to linguistics that are based on gradient computations and memorized constructions. I also describe why these models undermine strong claims for the innateness of language and respond to several critiques of large language models, including arguments that they can’t answer “why” questions and skepticism that they are informative about real life acquisition. Most notably, large language models have attained remarkable success at discovering grammar without using any of the methods that some in linguistics insisted were necessary for a science of language to progress. (UPDATED: With a postscript on replies to the original draft)},
	urldate = {2024-12-12},
	publisher = {LingBuzz},
	author = {Piantadosi, Steven},
	month = jul,
	year = {2024},
	note = {LingBuzz Published In: Edward Gibson \& Moshe Poliak (eds.), From fieldwork to linguistic theory: A tribute to Dan Everett (Empirically Oriented Theoretical Morphology and Syntax 15), 353–414. Berlin: Language Science Press. https : //doi.org/10.5281/zenodo.12665933.},
	keywords = {chomsky, cognitive science, computational modeling, emergent, generative syntax, large language model, minimalism, statistical learning, syntax},
}

@misc{kodner_why_2023,
	title = {Why {Linguistics} {Will} {Thrive} in the 21st {Century}: {A} {Reply} to {Piantadosi} (2023)},
	shorttitle = {Why {Linguistics} {Will} {Thrive} in the 21st {Century}},
	url = {http://arxiv.org/abs/2308.03228},
	doi = {10.48550/arXiv.2308.03228},
	abstract = {We present a critical assessment of Piantadosi's (2023) claim that "Modern language models refute Chomsky's approach to language," focusing on four main points. First, despite the impressive performance and utility of large language models (LLMs), humans achieve their capacity for language after exposure to several orders of magnitude less data. The fact that young children become competent, fluent speakers of their native languages with relatively little exposure to them is the central mystery of language learning to which Chomsky initially drew attention, and LLMs currently show little promise of solving this mystery. Second, what can the artificial reveal about the natural? Put simply, the implications of LLMs for our understanding of the cognitive structures and mechanisms underlying language and its acquisition are like the implications of airplanes for understanding how birds fly. Third, LLMs cannot constitute scientific theories of language for several reasons, not least of which is that scientific theories must provide interpretable explanations, not just predictions. This leads to our final point: to even determine whether the linguistic and cognitive capabilities of LLMs rival those of humans requires explicating what humans' capacities actually are. In other words, it requires a separate theory of language and cognition; generative linguistics provides precisely such a theory. As such, we conclude that generative linguistics as a scientific discipline will remain indispensable throughout the 21st century and beyond.},
	urldate = {2024-12-11},
	publisher = {arXiv},
	author = {Kodner, Jordan and Payne, Sarah and Heinz, Jeffrey},
	month = aug,
	year = {2023},
	note = {arXiv:2308.03228 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_growth_nodate,
	title = {The {Growth} {Of} {Logical} {Thinking} {From} {Childhood} {To} {Adolescence} : {Navajivan} {Prakashen} {Mandir} {Ahamadabad} : {Free} {Download}, {Borrow}, and {Streaming} : {Internet} {Archive}},
	url = {https://archive.org/details/in.ernet.dli.2015.190772/page/n3/mode/2up},
	urldate = {2024-12-11},
}

@book{inhelder_growth_1958,
	address = {New York, NY, US},
	series = {The growth of logical thinking:  {From} childhood to adolescence},
	title = {The growth of logical thinking:  {From} childhood to adolescence},
	shorttitle = {The growth of logical thinking},
	abstract = {This book has two aims: to set forth a description of changes in logical operations between childhood and adolescence and to describe the formal structures that mark the completion of the operational development of intelligence. To tie these together the authors have tried to present the material in a way that would stress the close relationship between the two. Each of the first fifteen chapters (Parts I and II) includes an experimental part by the first author and a brief final analysis by the second author. This analysis aims to isolate the formal or propositional structures found in each case. Chapters 16 and 17 (beginning of Part III) are the work of the second author, whereas Chapter 18 is a joint production. In addition, the specific problems of experimental induction analyzed from a functional standpoint (as distinguished from the present structural analysis) will be the subject of a special work by the first author. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Basic Books},
	author = {Inhelder, Bärbel and Piaget, Jean},
	editor = {Parsons, Anne and Milgram, Stanley},
	year = {1958},
	doi = {10.1037/10034-000},
	note = {Pages: xxvi, 378},
	keywords = {Adolescent Development, Childhood Development, Intellectual Development, Intelligence, Logical Thinking},
}

@book{piaget_origins_1952,
	title = {The origins of intelligence in children},
	language = {en},
	publisher = {International Universities Press, Inc},
	author = {Piaget, Jean},
	year = {1952},
}

@article{morse_why_2017,
	title = {Why {Are} {There} {Developmental} {Stages} in {Language} {Learning}? {A} {Developmental} {Robotics} {Model} of {Language} {Development}},
	volume = {41},
	issn = {1551-6709},
	shorttitle = {Why {Are} {There} {Developmental} {Stages} in {Language} {Learning}?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12390},
	doi = {10.1111/cogs.12390},
	abstract = {Most theories of learning would predict a gradual acquisition and refinement of skills as learning progresses, and while some highlight exponential growth, this fails to explain why natural cognitive development typically progresses in stages. Models that do span multiple developmental stages typically have parameters to “switch” between stages. We argue that by taking an embodied view, the interaction between learning mechanisms, the resulting behavior of the agent, and the opportunities for learning that the environment provides can account for the stage-wise development of cognitive abilities. We summarize work relevant to this hypothesis and suggest two simple mechanisms that account for some developmental transitions: neural readiness focuses on changes in the neural substrate resulting from ongoing learning, and perceptual readiness focuses on the perceptual requirements for learning new tasks. Previous work has demonstrated these mechanisms in replications of a wide variety of infant language experiments, spanning multiple developmental stages. Here we piece this work together as a single model of ongoing learning with no parameter changes at all. The model, an instance of the Epigenetic Robotics Architecture (Morse et al 2010) embodied on the iCub humanoid robot, exhibits ongoing multi-stage development while learning pre-linguistic and then basic language skills.},
	language = {en},
	number = {S1},
	urldate = {2024-12-11},
	journal = {Cognitive Science},
	author = {Morse, Anthony F. and Cangelosi, Angelo},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12390},
	keywords = {Cognitive architecture, Developmental robotics, Language learning, Ongoing development},
	pages = {32--51},
}

@article{goh_multimodal_2021,
	title = {Multimodal {Neurons} in {Artificial} {Neural} {Networks}},
	volume = {6},
	issn = {2476-0757},
	url = {https://distill.pub/2021/multimodal-neurons},
	doi = {10.23915/distill.00030},
	number = {3},
	urldate = {2024-11-27},
	journal = {Distill},
	author = {Goh, Gabriel and Cammarata, Nick and Voss, Chelsea and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
	month = mar,
	year = {2021},
	pages = {10.23915/distill.00030},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kitaev_reformer_2020,
	title = {Reformer: {The} {Efficient} {Transformer}},
	shorttitle = {Reformer},
	url = {http://arxiv.org/abs/2001.04451},
	doi = {10.48550/arXiv.2001.04451},
	abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L{\textasciicircum}2\$) to O(\$L{\textbackslash}log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm},
	month = feb,
	year = {2020},
	note = {arXiv:2001.04451},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gukov_learning_2020,
	title = {Learning to {Unknot}},
	url = {http://arxiv.org/abs/2010.16263},
	doi = {10.48550/arXiv.2010.16263},
	abstract = {We introduce natural language processing into the study of knot theory, as made natural by the braid word representation of knots. We study the UNKNOT problem of determining whether or not a given knot is the unknot. After describing an algorithm to randomly generate \$N\$-crossing braids and their knot closures and discussing the induced prior on the distribution of knots, we apply binary classification to the UNKNOT decision problem. We find that the Reformer and shared-QK Transformer network architectures outperform fully-connected networks, though all perform well. Perhaps surprisingly, we find that accuracy increases with the length of the braid word, and that the networks learn a direct correlation between the confidence of their predictions and the degree of the Jones polynomial. Finally, we utilize reinforcement learning (RL) to find sequences of Markov moves and braid relations that simplify knots and can identify unknots by explicitly giving the sequence of unknotting actions. Trust region policy optimization (TRPO) performs consistently well for a wide range of crossing numbers and thoroughly outperformed other RL algorithms and random walkers. Studying these actions, we find that braid relations are more useful in simplifying to the unknot than one of the Markov moves.},
	urldate = {2024-11-24},
	publisher = {arXiv},
	author = {Gukov, Sergei and Halverson, James and Ruehle, Fabian and Sułkowski, Piotr},
	month = oct,
	year = {2020},
	note = {arXiv:2010.16263},
	keywords = {Computer Science - Machine Learning, High Energy Physics - Theory, Mathematics - Geometric Topology},
}

@misc{li_what_2024,
	title = {What {Happened} in {LLMs} {Layers} when {Trained} for {Fast} vs. {Slow} {Thinking}: {A} {Gradient} {Perspective}},
	shorttitle = {What {Happened} in {LLMs} {Layers} when {Trained} for {Fast} vs. {Slow} {Thinking}},
	url = {http://arxiv.org/abs/2410.23743},
	doi = {10.48550/arXiv.2410.23743},
	abstract = {What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. Our code, data, and gradient statistics can be found in: https://github.com/MingLiiii/Layer\_Gradient.},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Li, Ming and Li, Yanhong and Zhou, Tianyi},
	month = oct,
	year = {2024},
	note = {arXiv:2410.23743},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kang_what_2024,
	title = {What {Do} {Learning} {Dynamics} {Reveal} {About} {Generalization} in {LLM} {Reasoning}?},
	url = {http://arxiv.org/abs/2411.07681},
	doi = {10.48550/arXiv.2411.07681},
	abstract = {Despite the remarkable capabilities of modern large language models (LLMs), the mechanisms behind their problem-solving abilities remain elusive. In this work, we aim to better understand how the learning dynamics of LLM finetuning shapes downstream generalization. Our analysis focuses on reasoning tasks, whose problem structure allows us to distinguish between memorization (the exact replication of reasoning steps from the training data) and performance (the correctness of the final solution). We find that a model's generalization behavior can be effectively characterized by a training metric we call pre-memorization train accuracy: the accuracy of model samples on training queries before they begin to copy the exact reasoning steps from the training set. On the dataset level, this metric is able to reliably predict test accuracy, achieving \$R{\textasciicircum}2\$ of around or exceeding 0.9 across various models (Llama3 8, Gemma2 9B), datasets (GSM8k, MATH), and training configurations. On a per-example level, this metric is also indicative of whether individual model predictions are robust to perturbations in the training query. By connecting a model's learning behavior to its generalization, pre-memorization train accuracy can guide targeted improvements to training strategies. We focus on data curation as an example, and show that prioritizing examples with low pre-memorization accuracy leads to 1.5-2x improvements in data efficiency compared to i.i.d. data scaling, and outperforms other standard data curation techniques.},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Kang, Katie and Setlur, Amrith and Ghosh, Dibya and Steinhardt, Jacob and Tomlin, Claire and Levine, Sergey and Kumar, Aviral},
	month = nov,
	year = {2024},
	note = {arXiv:2411.07681},
	keywords = {Computer Science - Machine Learning},
}

@misc{an_learning_2024,
	title = {Learning {From} {Mistakes} {Makes} {LLM} {Better} {Reasoner}},
	url = {http://arxiv.org/abs/2310.20689},
	doi = {10.48550/arXiv.2310.20689},
	abstract = {Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a ''corrector'' to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning. Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data. These results suggest a significant potential for LLMs to improve through learning from their mistakes. Our code, models and prompts are publicly available at https://github.com/microsoft/LEMA.},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {An, Shengnan and Ma, Zexiong and Lin, Zeqi and Zheng, Nanning and Lou, Jian-Guang and Chen, Weizhu},
	month = mar,
	year = {2024},
	note = {arXiv:2310.20689},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{vong_grounded_2024,
	title = {Grounded language acquisition through the eyes and ears of a single child},
	volume = {383},
	url = {https://www.science.org/doi/10.1126/science.adi1374},
	doi = {10.1126/science.adi1374},
	abstract = {Starting around 6 to 9 months of age, children begin acquiring their first words, linking spoken words to their visual counterparts. How much of this knowledge is learnable from sensory input with relatively generic learning mechanisms, and how much requires stronger inductive biases? Using longitudinal head-mounted camera recordings from one child aged 6 to 25 months, we trained a relatively generic neural network on 61 hours of correlated visual-linguistic data streams, learning feature-based representations and cross-modal associations. Our model acquires many word-referent mappings present in the child’s everyday experience, enables zero-shot generalization to new visual referents, and aligns its visual and linguistic conceptual systems. These results show how critical aspects of grounded word meaning are learnable through joint representation and associative learning from one child’s input.},
	number = {6682},
	urldate = {2024-11-22},
	journal = {Science},
	author = {Vong, Wai Keen and Wang, Wentao and Orhan, A. Emin and Lake, Brenden M.},
	month = feb,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {504--511},
}

@article{gilkerson_mapping_2017,
	title = {Mapping the {Early} {Language} {Environment} {Using} {All}-{Day} {Recordings} and {Automated} {Analysis}},
	volume = {26},
	url = {https://pubs.asha.org/doi/10.1044/2016_AJSLP-15-0169},
	doi = {10.1044/2016_AJSLP-15-0169},
	abstract = {PurposeThis research provided a first-generation standardization of automated language environment estimates, validated these estimates against standard language assessments, and extended on previous research reporting language behavior differences across socioeconomic groups.MethodTypically developing children between 2 to 48 months of age completed monthly, daylong recordings in their natural language environments over a span of approximately 6–38 months. The resulting data set contained 3,213 12-hr recordings automatically analyzed by using the Language Environment Analysis (LENA) System to generate estimates of (a) the number of adult words in the child's environment, (b) the amount of caregiver–child interaction, and (c) the frequency of child vocal output.ResultsChild vocalization frequency and turn-taking increased with age, whereas adult word counts were age independent after early infancy. Child vocalization and conversational turn estimates predicted 7\%–16\% of the variance observed in child language assessment scores. Lower socioeconomic status (SES) children produced fewer vocalizations, engaged in fewer adult–child interactions, and were exposed to fewer daily adult words compared with their higher socioeconomic status peers, but within-group variability was high.ConclusionsThe results offer new insight into the landscape of the early language environment, with clinical implications for identification of children at-risk for impoverished language environments.},
	number = {2},
	urldate = {2024-11-22},
	journal = {American Journal of Speech-Language Pathology},
	author = {Gilkerson, Jill and Richards, Jeffrey A. and Warren, Steven F. and Montgomery, Judith K. and Greenwood, Charles R. and Kimbrough Oller, D. and Hansen, John H. L. and Paul, Terrance D.},
	month = may,
	year = {2017},
	note = {Publisher: American Speech-Language-Hearing Association},
	pages = {248--265},
}

@misc{nguyen_democratizing_2024,
	title = {Democratizing {LLMs} for {Low}-{Resource} {Languages} by {Leveraging} their {English} {Dominant} {Abilities} with {Linguistically}-{Diverse} {Prompts}},
	url = {http://arxiv.org/abs/2306.11372},
	doi = {10.48550/arXiv.2306.11372},
	abstract = {Large language models (LLMs) are known to effectively perform tasks by simply observing few exemplars. However, in low-resource languages, obtaining such hand-picked exemplars can still be challenging, where unsupervised techniques may be necessary. Moreover, competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs' ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages to prompt the LLMs to translate from any language into English. These prompts are then used to create intra-lingual exemplars to perform tasks in the target languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 13 Indic and 21 African low-resource languages. We also show that fine-tuning a 7B model on data generated from our method helps it perform competitively with a 175B model. In non-English translation tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many low-resource languages. When evaluated on zero-shot multilingual summarization, our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is also favored by GPT-4.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Nguyen, Xuan-Phi and Aljunied, Sharifah Mahani and Joty, Shafiq and Bing, Lidong},
	month = jul,
	year = {2024},
	note = {arXiv:2306.11372},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{mercier_why_2011,
	title = {Why do humans reason? {Arguments} for an argumentative theory},
	volume = {34},
	issn = {1469-1825, 0140-525X},
	shorttitle = {Why do humans reason?},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/why-do-humans-reason-arguments-for-an-argumentative-theory/53E3F3180014E80E8BE9FB7A2DD44049},
	doi = {10.1017/S0140525X10000968},
	abstract = {Reasoning is generally seen as a means to improve knowledge and make better decisions. However, much evidence shows that reasoning often leads to epistemic distortions and poor decisions. This suggests that the function of reasoning should be rethought. Our hypothesis is that the function of reasoning is argumentative. It is to devise and evaluate arguments intended to persuade. Reasoning so conceived is adaptive given the exceptional dependence of humans on communication and their vulnerability to misinformation. A wide range of evidence in the psychology of reasoning and decision making can be reinterpreted and better explained in the light of this hypothesis. Poor performance in standard reasoning tasks is explained by the lack of argumentative context. When the same problems are placed in a proper argumentative setting, people turn out to be skilled arguers. Skilled arguers, however, are not after the truth but after arguments supporting their views. This explains the notorious confirmation bias. This bias is apparent not only when people are actually arguing, but also when they are reasoning proactively from the perspective of having to defend their opinions. Reasoning so motivated can distort evaluations and attitudes and allow erroneous beliefs to persist. Proactively used reasoning also favors decisions that are easy to justify but not necessarily better. In all these instances traditionally described as failures or flaws, reasoning does exactly what can be expected of an argumentative device: Look for arguments that support a given conclusion, and, ceteris paribus, favor conclusions for which arguments can be found.},
	language = {en},
	number = {2},
	urldate = {2024-11-18},
	journal = {Behavioral and Brain Sciences},
	author = {Mercier, Hugo and Sperber, Dan},
	month = apr,
	year = {2011},
	keywords = {argumentation, confirmation bias, decision making, dual process theory, evolutionary psychology, motivated reasoning, reason-based choice, reasoning},
	pages = {57--74},
}

@article{cheng_pragmatic_1986,
	title = {Pragmatic versus syntactic approaches to training deductive reasoning},
	copyright = {IndexNoFollow},
	issn = {0010-0285},
	url = {http://deepblue.lib.umich.edu/handle/2027.42/26121},
	abstract = {Two views have dominated theories of deductive reasoning. One is the view that people reason using syntactic, domain-independent rules of logic, and the other is the view that people use domain-specific knowledge. In contrast with both of these views, we present evidence that people often reason using a type of knowledge structure termed pragmatic reasoning schemas. In two experiments, syntactically equivalent forms of conditional rules produced different patterns of performance in Wason's selection task, depending on the type of pragmatic schema evoked. The differences could not be explained by either dominant view. We further tested the syntactic view by manipulating the type of logic training subjects received. If people typically do not use abstract rules analogous to those of standard logic, then training on abstract principles of standard logic alone would have little effect on selection performance, because the subjects would not know how to map such rules onto concrete instances. Training results obtained in both a laboratory and a classroom setting confirmed our hypothesis: Training was effective only when abstract principles were coupled with examples of selection problems, which served to elucidate the mapping between abstract principles and concrete instances. In contrast, a third experiment demonstrated that brief abstract training on a pragmatic reasoning schema had a substantial impact on subjects' reasoning about problems that were interpretable in terms of the schema. The dominance of pragmatic schemas over purely syntactic rules was discussed with respect to the relative utility of both types of rules for solving real-world problems.},
	language = {en\_US},
	urldate = {2024-11-18},
	author = {Cheng, Patricia W. and Holyoak, Keith J. and Nisbett, Richard E. and Oliver, Lindsay M.},
	month = jul,
	year = {1986},
	note = {Accepted: 2006-04-07T19:29:16Z
Publisher: Elsevier},
}

@incollection{bates_individual_1996,
	title = {Individual {Differences} and their {Implications} for {Theories} of {Language} {Development}},
	copyright = {© 1995 Blackwell Publishing Ltd.},
	isbn = {978-1-4051-6631-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/b.9780631203124.1996.00005.x},
	abstract = {Like every other aspect of human development, language development is characterized by variation. Historically this variation has been largely ignored by students of child language, who have concentrated on the remarkable similarities in sequence of development that are usually observed across children acquiring a given language. Individual differences in rate of development and individual differences in learning style have been left to applied practitioners such as speech pathologists and special educators. We believe it is no accident that these professionals, concerned with such important questions as the definition of abnormality, the relationship of language to nonverbal cognition, and the role of environmental variables, have found it essential to focus on variation.},
	language = {en},
	urldate = {2024-11-18},
	booktitle = {The {Handbook} of {Child} {Language}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Bates, Elizabeth and Dale, Philip S. and Thal, Donna},
	year = {1996},
	doi = {10.1111/b.9780631203124.1996.00005.x},
	note = {Section: 4
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/b.9780631203124.1996.00005.x},
	keywords = {atypical children, fault lines, individual differences, variations},
	pages = {95--151},
}

@article{mccarthy_language_1960,
	title = {Language {Development}},
	volume = {25},
	issn = {0037-976X},
	url = {https://www.jstor.org/stable/1165630},
	doi = {10.2307/1165630},
	number = {3},
	urldate = {2024-11-18},
	journal = {Monographs of the Society for Research in Child Development},
	author = {McCarthy, Dorothea},
	year = {1960},
	note = {Publisher: [Society for Research in Child Development, Wiley]},
	pages = {5--14},
}

@incollection{mccarthy_language_1946,
	address = {Hoboken, NJ, US},
	title = {Language development in children},
	abstract = {Reviews the literature on language development in children. The following topics are discussed: characteristics of the literature, developmental stages, the growth of vocabulary, comprehensibility of speech, quantitative measures, sentence structure and grammatical form, the functions of language in the child's life, interrelationships of various measures of language development, the relation of language to other aspects of development, language development and individual differences, and the effect of various environmental factors. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	booktitle = {Manual of child psychology},
	publisher = {John Wiley \& Sons, Inc.},
	author = {McCarthy, Dorothea},
	year = {1946},
	doi = {10.1037/10756-010},
	keywords = {Childhood Development, Language Development},
	pages = {476--581},
}

@inproceedings{waldon_modeling_2020,
	address = {New York, New York},
	title = {Modeling {Behavior} in {Truth} {Value} {Judgment} {Task} {Experiments}},
	url = {https://aclanthology.org/2020.scil-1.29},
	urldate = {2024-11-18},
	booktitle = {Proceedings of the {Society} for {Computation} in {Linguistics} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Waldon, Brandon and Degen, Judith},
	editor = {Ettinger, Allyson and Jarosz, Gaja and Pater, Joe},
	month = jan,
	year = {2020},
	pages = {238--247},
}

@inproceedings{kilian_note_1992,
	address = {New York, NY, USA},
	series = {{STOC} '92},
	title = {A note on efficient zero-knowledge proofs and arguments (extended abstract)},
	isbn = {978-0-89791-511-3},
	url = {https://dl.acm.org/doi/10.1145/129712.129782},
	doi = {10.1145/129712.129782},
	abstract = {In this note, we present new zero-knowledge interactive proofs and arguments for languages in NP. To show that x ε L, with an error probability of at most 2-k, our zero-knowledge proof system requires O({\textbar}x{\textbar}c1)+O(lgc2{\textbar}x{\textbar})k ideal bit commitments, where c1 and c2 depend only on L. This construction is the first in the ideal bit commitment model that achieves large values of k more efficiently than by running  k independent iterations of the base interactive proof system. Under suitable complexity assumptions, we exhibit  zero knowledge arguments that require O(lgc{\textbar}x{\textbar}kl bits of communication, where c depends only on L, and l is the security parameter for the prover. This is the first construction in which the total amount of communication can be less than that needed to transmit the NP witness. Our protocols are based on efficiently checkable proofs for NP[4].},
	urldate = {2024-11-18},
	booktitle = {Proceedings of the twenty-fourth annual {ACM} symposium on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Kilian, Joe},
	month = jul,
	year = {1992},
	pages = {723--732},
}

@misc{sun_zkllm_2024,
	title = {{zkLLM}: {Zero} {Knowledge} {Proofs} for {Large} {Language} {Models}},
	shorttitle = {{zkLLM}},
	url = {http://arxiv.org/abs/2404.16109},
	doi = {10.48550/arXiv.2404.16109},
	abstract = {The recent surge in artificial intelligence (AI), characterized by the prominence of large language models (LLMs), has ushered in fundamental transformations across the globe. However, alongside these advancements, concerns surrounding the legitimacy of LLMs have grown, posing legal challenges to their extensive applications. Compounding these concerns, the parameters of LLMs are often treated as intellectual property, restricting direct investigations. In this study, we address a fundamental challenge within the realm of AI legislation: the need to establish the authenticity of outputs generated by LLMs. To tackle this issue, we present zkLLM, which stands as the inaugural specialized zero-knowledge proof tailored for LLMs to the best of our knowledge. Addressing the persistent challenge of non-arithmetic operations in deep learning, we introduce tlookup, a parallelized lookup argument designed for non-arithmetic tensor operations in deep learning, offering a solution with no asymptotic overhead. Furthermore, leveraging the foundation of tlookup, we introduce zkAttn, a specialized zero-knowledge proof crafted for the attention mechanism, carefully balancing considerations of running time, memory usage, and accuracy. Empowered by our fully parallelized CUDA implementation, zkLLM emerges as a significant stride towards achieving efficient zero-knowledge verifiable computations over LLMs. Remarkably, for LLMs boasting 13 billion parameters, our approach enables the generation of a correctness proof for the entire inference process in under 15 minutes. The resulting proof, compactly sized at less than 200 kB, is designed to uphold the privacy of the model parameters, ensuring no inadvertent information leakage.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Sun, Haochen and Li, Jason and Zhang, Hongyang},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16109},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{lightman_lets_2023,
	title = {Let's {Verify} {Step} by {Step}},
	url = {http://arxiv.org/abs/2305.20050},
	doi = {10.48550/arXiv.2305.20050},
	abstract = {In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78\% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
	month = may,
	year = {2023},
	note = {arXiv:2305.20050},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{gukov_learning_2020-1,
	title = {Learning to {Unknot}},
	url = {http://arxiv.org/abs/2010.16263},
	abstract = {We introduce natural language processing into the study of knot theory, as made natural by the braid word representation of knots. We study the UNKNOT problem of determining whether or not a given knot is the unknot. After describing an algorithm to randomly generate \$N\$-crossing braids and their knot closures and discussing the induced prior on the distribution of knots, we apply binary classification to the UNKNOT decision problem. We find that the Reformer and shared-QK Transformer network architectures outperform fully-connected networks, though all perform well. Perhaps surprisingly, we find that accuracy increases with the length of the braid word, and that the networks learn a direct correlation between the confidence of their predictions and the degree of the Jones polynomial. Finally, we utilize reinforcement learning (RL) to find sequences of Markov moves and braid relations that simplify knots and can identify unknots by explicitly giving the sequence of unknotting actions. Trust region policy optimization (TRPO) performs consistently well for a wide range of crossing numbers and thoroughly outperformed other RL algorithms and random walkers. Studying these actions, we find that braid relations are more useful in simplifying to the unknot than one of the Markov moves.},
	urldate = {2024-11-15},
	author = {Gukov, Sergei and Halverson, James and Ruehle, Fabian and Sułkowski, Piotr},
	month = oct,
	year = {2020},
	note = {arXiv:2010.16263},
	keywords = {Computer Science - Machine Learning, High Energy Physics - Theory, Mathematics - Geometric Topology},
}

@article{argerich_measuring_2024,
	title = {Measuring and {Improving} the {Energy} {Efficiency} of {Large} {Language} {Models} {Inference}},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10549890/?arnumber=10549890},
	doi = {10.1109/ACCESS.2024.3409745},
	abstract = {Recent improvements in the accuracy of machine learning (ML) models in the language domain have propelled their use in a multitude of products and services, touching millions of lives daily. These new levels of accuracy have been attained mainly through exponential growth in model size, creating a new category of models known as Large Language Models (LLMs) and leading to a substantial increase in computing and energy demands. While recent studies have focused on measuring and improving the energy consumption of LLMs during training, inference has received little attention. In this article, we present an approach to profile the energy consumption of LLMs during inference and leverage it to improve energy efficiency. For this, we deploy several state-of-the-art LLMs and observe how model size, number of layers, parallelized attention, and even vocabulary size affect their energy consumption. In addition, we leverage input batch size and different quantization levels to optimize their inference energy efficiency and latency.},
	urldate = {2024-11-15},
	journal = {IEEE Access},
	author = {Argerich, Mauricio Fadel and Patiño-Martínez, Marta},
	year = {2024},
	note = {Conference Name: IEEE Access},
	keywords = {Computational modeling, Energy consumption, Energy measurement, Graphics processing units, Large language models, Machine learning, Software measurement, Training, deep learning, energy efficiency, large language models, machine learning},
	pages = {80194--80207},
}

@article{knox_augmenting_nodate,
	title = {Augmenting {Reinforcement} {Learning} with {Human} {Feedback}},
	abstract = {As computational agents are increasingly used beyond research labs, their success will depend on their ability to learn new skills and adapt to their dynamic, complex environments. If human users — without programming skills — can transfer their task knowledge to agents, learning can accelerate dramatically, reducing costly trials. The TAMER framework guides the design of agents whose behavior can be shaped through signals of approval and disapproval, a natural form of human feedback. More recently, TAMER+RL was introduced to enable human feedback to augment a traditional reinforcement learning (RL) agent that learns from a Markov decision process’s (MDP) reward signal. Using a reimplementation of TAMER and TAMER+RL, we address limitations of prior work, contributing in two critical directions. First, the four successful techniques for combining a human reinforcement with RL from prior TAMER+RL work are tested on a second task, and these techniques’ sensitivities to parameter changes are analyzed. Together, these examinations yield more general and prescriptive conclusions to guide others who wish to incorporate human knowledge into an RL algorithm. Second, TAMER+RL has thus far been limited to a sequential setting, in which training occurs before learning from MDP reward. We modify the sequential algorithms to learn simultaneously from both sources, enabling the human feedback to come at any time during the reinforcement learning process. To enable simultaneous learning, we introduce a new technique that appropriately determines the magnitude of the human model’s inﬂuence on the RL algorithm throughout time and state-action space.},
	language = {en},
	author = {Knox, W Bradley and Stone, Peter},
}

@article{pauls_u-shaped_2013,
	title = {U-{Shaped} {Development}: {An} {Old} but {Unsolved} {Problem}},
	volume = {4},
	shorttitle = {U-{Shaped} {Development}},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC3664325/},
	doi = {10.3389/fpsyg.2013.00301},
	abstract = {Even today the investigation of U-shaped functions in human development is of considerable importance for different domains of Developmental Psychology. More and more scientific researchers focus their efforts on the challenge to describe and ...},
	language = {en},
	urldate = {2024-11-15},
	journal = {Frontiers in Psychology},
	author = {Pauls, Franz and Macha, Thorsten and Petermann, Franz},
	month = may,
	year = {2013},
	pmid = {23750146},
	pages = {301},
}

@article{plunkett_u-shaped_1991,
	title = {U-shaped learning and frequency effects in a multi-layered perception: {Implications} for child language acquisition},
	volume = {38},
	issn = {0010-0277},
	shorttitle = {U-shaped learning and frequency effects in a multi-layered perception},
	url = {https://www.sciencedirect.com/science/article/pii/001002779190022V},
	doi = {10.1016/0010-0277(91)90022-V},
	abstract = {A three-layer back-propagation network is used to implement a pattern association task in which four types of mapping are learned. These mappings, which are considered analogous to those which characterize the relationship between the stem and past tense forms of English verbs, include arbitrary mappings, identity mappings, vowel changes, and additions of a suffix. The degree of correspondence between parallel distributed processing (PDP) models which learn mappings of this sort (e.g., Rumelhart \& McClelland, 1986, 1987) and children's acquisition of inflectional morphology has recently been at issue in discussions of the applicability of PDP models to the study of human cognition and language (Pinker \& Mehler, 1989; Bever, in press). In this paper, we explore the capacity of a network to learn these types of mappings, focusing on three major issues. First, we compare the performance of a single-layered perceptron similar to the one used by Rumerlhart and McClelland with a multi-layered perceptron. The results suggest that it is unlikely that a single-layered perceptron is capable of finding an adequate solution to the problem of mapping stems and past tense forms in input configurations that are sufficiently analogous to English. Second, we explore the input conditions which determine learning in these networks. Several factors that characterize linguistic input are investigated: (a) the nature of the mapping performed by the network (arbitrary, suffixation, identity, and vowel change); (b) the competition effects that arise when the task demands simultaneous learning of distinct mapping types; (c) the role of the type and token frequency of verb stems; and (d) the influence of phonological subregularities in the irregular verbs. Each of these factors is shown to have selective consequences on both successful and erroneous performance in the network. Third, we outline several types of systems which could result in U-shaped acquisition, and discuss the ways in which learning in multi-layered networks can be seen to capture several characteristics of U-shaped learning in children. In general, these models provide information about the role of input in determining the kinds of errors that a network will produce, including the conditions under which rule-like behavior and U-shaped learning will and will not emerge. The results from all simulations are discussed in light of behavioral data on children's acquisition of the past tense and the validity of drawing conclusions about the acquisition of language from models of this sort.},
	number = {1},
	urldate = {2024-11-15},
	journal = {Cognition},
	author = {Plunkett, Kim and Marchman, Virginia},
	month = jan,
	year = {1991},
	pages = {43--102},
}

@article{lau_grammaticality_2017,
	title = {Grammaticality, {Acceptability}, and {Probability}: {A} {Probabilistic} {View} of {Linguistic} {Knowledge}},
	volume = {41},
	copyright = {Copyright © 2016 Cognitive Science Society, Inc.},
	issn = {1551-6709},
	shorttitle = {Grammaticality, {Acceptability}, and {Probability}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12414},
	doi = {10.1111/cogs.12414},
	abstract = {The question of whether humans represent grammatical knowledge as a binary condition on membership in a set of well-formed sentences, or as a probabilistic property has been the subject of debate among linguists, psychologists, and cognitive scientists for many decades. Acceptability judgments present a serious problem for both classical binary and probabilistic theories of grammaticality. These judgements are gradient in nature, and so cannot be directly accommodated in a binary formal grammar. However, it is also not possible to simply reduce acceptability to probability. The acceptability of a sentence is not the same as the likelihood of its occurrence, which is, in part, determined by factors like sentence length and lexical frequency. In this paper, we present the results of a set of large-scale experiments using crowd-sourced acceptability judgments that demonstrate gradience to be a pervasive feature in acceptability judgments. We then show how one can predict acceptability judgments on the basis of probability by augmenting probabilistic language models with an acceptability measure. This is a function that normalizes probability values to eliminate the confounding factors of length and lexical frequency. We describe a sequence of modeling experiments with unsupervised language models drawn from state-of-the-art machine learning methods in natural language processing. Several of these models achieve very encouraging levels of accuracy in the acceptability prediction task, as measured by the correlation between the acceptability measure scores and mean human acceptability values. We consider the relevance of these results to the debate on the nature of grammatical competence, and we argue that they support the view that linguistic knowledge can be intrinsically probabilistic.},
	language = {en},
	number = {5},
	urldate = {2024-11-15},
	journal = {Cognitive Science},
	author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12414},
	keywords = {Grammaticality, Probabilistic modeling, Syntactic knowledge},
	pages = {1202--1241},
}

@article{belinkov_probing_2022,
	title = {Probing {Classifiers}: {Promises}, {Shortcomings}, and {Advances}},
	volume = {48},
	issn = {0891-2017},
	shorttitle = {Probing {Classifiers}},
	url = {https://doi.org/10.1162/coli_a_00422},
	doi = {10.1162/coli_a_00422},
	abstract = {Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple—a classifier is trained to predict some linguistic property from a model’s representations—and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.},
	number = {1},
	urldate = {2024-11-14},
	journal = {Computational Linguistics},
	author = {Belinkov, Yonatan},
	month = apr,
	year = {2022},
	pages = {207--219},
}

@article{linzen_assessing_2016,
	title = {Assessing the {Ability} of {LSTMs} to {Learn} {Syntax}-{Sensitive} {Dependencies}},
	volume = {4},
	url = {https://aclanthology.org/Q16-1037},
	doi = {10.1162/tacl_a_00115},
	abstract = {The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1\% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.},
	urldate = {2024-11-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	editor = {Lee, Lillian and Johnson, Mark and Toutanova, Kristina},
	year = {2016},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {521--535},
}

@misc{he_deberta_2021,
	title = {{DeBERTa}: {Decoding}-enhanced {BERT} with {Disentangled} {Attention}},
	shorttitle = {{DeBERTa}},
	url = {http://arxiv.org/abs/2006.03654},
	doi = {10.48550/arXiv.2006.03654},
	abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2006.03654},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - undefined},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{wang_glue_2018,
	address = {Brussels, Belgium},
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://aclweb.org/anthology/W18-5446},
	doi = {10.18653/v1/W18-5446},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and ﬁnd that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
	language = {en},
	urldate = {2024-11-14},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	year = {2018},
	pages = {353--355},
}

@misc{noauthor_palm_nodate,
	title = {{PaLM}: scaling language modeling with pathways: {The} {Journal} of {Machine} {Learning} {Research}: {Vol} 24, {No} 1},
	url = {https://dl.acm.org/doi/10.5555/3648699.3648939},
	urldate = {2024-11-14},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2024-11-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}

@book{chomsky_aspects_1965,
	edition = {50},
	title = {Aspects of the theory of syntax},
	isbn = {978-0-262-52740-8},
	url = {https://www.jstor.org/stable/j.ctt17kk81z},
	abstract = {Noam Chomsky's \textit{Aspects of the Theory of Syntax} , published in 1965, was a landmark work in generative grammar that introduced certain technical innovations still drawn upon in contemporary work. The fiftieth anniversary edition of this influential book includes a new preface by the author that identifies proposals that seem to be of lasting significance, reviews changes and improvements in the formulation and implementation of basic ideas, and addresses some of the controversies that arose over the general framework.Beginning in the mid-fifties and emanating largely from MIT, linguists developed an approach to linguistic theory and to the study of the structure of particular languages that diverged in many respects from conventional modern linguistics. Although the new approach was connected to the traditional study of languages, it differed enough in its specific conclusions about the structure of language to warrant a name, "generative grammar." Various deficiencies were discovered in the first attempts to formulate a theory of transformational generative grammar and in the descriptive analysis of particular languages that motivated these formulations. At the same time, it became apparent that these formulations can be extended and deepened. In this book, Chomsky reviews these developments and proposes a reformulation of the theory of transformational generative grammar that takes them into account. The emphasis in this study is syntax; semantic and phonological aspects of the language structure are discussed only insofar as they bear on syntactic theory.},
	urldate = {2024-11-14},
	publisher = {The MIT Press},
	author = {Chomsky, Noam},
	year = {1965},
}

@article{chomsky_aspects_nodate,
	title = {{ASPECTS} {OF} {THE} {THEORY} {OF} {SYNTAX}},
	language = {en},
	author = {Chomsky, Noam},
}

@book{chomsky_syntactic_1957,
	address = {Oxford, England},
	series = {Syntactic structures},
	title = {Syntactic structures},
	abstract = {A considerable portion of this monograph is devoted to the scientific and logical status of the theory of grammar, e.g., the problem of whether it is possible to set forth a mechanical procedure for discovering the grammar of a language. The remainder is devoted to a comparison and analysis of several possible types of grammatical models, in particular the information theory model, the "immediate constituent" model, and the kernel-transformation model. The last of these is found most acceptable and is explicated in terms of English grammar. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Mouton},
	author = {Chomsky, Noem},
	year = {1957},
	note = {Pages: 116},
}

@misc{noauthor_syntactic_nodate,
	title = {Syntactic {Structures}: {Chomsky}, {Noam}: 9781614278047: {Amazon}.com: {Books}},
	url = {https://www.amazon.com/Syntactic-Structures-Noam-Chomsky/dp/1614278040},
	urldate = {2024-11-14},
}

@article{chomsky_syntactic_nodate,
	title = {Syntactic {Structures}},
	language = {en},
	author = {Chomsky, Noam},
}

@misc{warstadt_what_2024,
	title = {What {Artificial} {Neural} {Networks} {Can} {Tell} {Us} {About} {Human} {Language} {Acquisition}},
	url = {http://arxiv.org/abs/2208.07998},
	doi = {10.48550/arXiv.2208.07998},
	abstract = {Rapid progress in machine learning for natural language processing has the potential to transform debates about how humans learn language. However, the learning environments and biases of current artificial learners and humans diverge in ways that weaken the impact of the evidence obtained from learning simulations. For example, today's most effective neural language models are trained on roughly one thousand times the amount of linguistic data available to a typical child. To increase the relevance of learnability results from computational models, we need to train model learners without significant advantages over humans. If an appropriate model successfully acquires some target linguistic knowledge, it can provide a proof of concept that the target is learnable in a hypothesized human learning scenario. Plausible model learners will enable us to carry out experimental manipulations to make causal inferences about variables in the learning environment, and to rigorously test poverty-of-the-stimulus-style claims arguing for innate linguistic knowledge in humans on the basis of speculations about learnability. Comparable experiments will never be possible with human subjects due to practical and ethical considerations, making model learners an indispensable resource. So far, attempts to deprive current models of unfair advantages obtain sub-human results for key grammatical behaviors such as acceptability judgments. But before we can justifiably conclude that language learning requires more prior domain-specific knowledge than current models possess, we must first explore non-linguistic inputs in the form of multimodal stimuli and multi-agent interaction as ways to make our learners more efficient at learning from limited linguistic input.},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Warstadt, Alex and Bowman, Samuel R.},
	month = feb,
	year = {2024},
	note = {arXiv:2208.07998},
	keywords = {Computer Science - Computation and Language},
}

@article{krafft_levels_nodate,
	title = {Levels of {Analysis} in {Computational} {Social} {Science}},
	abstract = {Marr’s levels of analysis constitute one inﬂuential approach to the central program of cognitive science—the multilevel analysis of cognition as information processing. The distinctive aspects of Marr’s framework are an emphasis on identifying the computational problems and constraints faced in cognition, and conceptual machinery to relate cognitive mechanisms to that computational level of analysis. Although related ideas have been explored in a range of social science disciplines, Marr’s framework, and particularly its notion of the precise formulation of computational problems and solutions, has yet to be applied widely in social analysis. In the present work we develop a formulation of Marr’s levels for social systems, provide examples of this approach, and address potential criticisms. The consequence is a computational perspective on the sociological school of structural functionalism, and an apparatus for conducting multiscale analysis of social systems.},
	language = {en},
	author = {Krafft, Peter M and Grifﬁths, Thomas L},
}

@inproceedings{linzen_how_2020,
	address = {Online},
	title = {How can we accelerate progress towards human-like linguistic generalization?},
	url = {https://aclanthology.org/2020.acl-main.465},
	doi = {10.18653/v1/2020.acl-main.465},
	abstract = {This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.},
	urldate = {2024-10-18},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Linzen, Tal},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {5210--5217},
}

@misc{mirzadeh_gsm-symbolic_2024,
	title = {{GSM}-{Symbolic}: understanding the limitations of mathematical reasoning in large language models},
	shorttitle = {{GSM}-{Symbolic}},
	url = {http://arxiv.org/abs/2410.05229},
	doi = {10.48550/arXiv.2410.05229},
	abstract = {Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65\%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.},
	urldate = {2024-10-18},
	publisher = {arXiv},
	author = {Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
	month = oct,
	year = {2024},
	note = {arXiv:2410.05229},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{portelance_roles_2024,
	title = {The roles of neural networks in language acquisition},
	volume = {18},
	issn = {1749-818X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.70001},
	doi = {10.1111/lnc3.70001},
	abstract = {How can modern neural networks like language models be useful to the field of language acquisition, and more broadly cognitive science, if they are not a priori designed to be cognitive models? As developments towards natural language understanding and generation have improved leaps and bounds, with models like GPT-4, the question of how they can inform our understanding of human language acquisition has re-emerged. As such, it is critical to examine how in practice linking hypotheses between models and human learners can be safely established. To address these questions, we propose a model taxonomy, including four modelling approaches, each having differing goals, from exploratory hypothesis generation to hypothesis differentiation and testing. We show how the goals of these approaches align with the overarching goals of science and linguistics by connecting our taxonomy to the realist versus instrumentalist approaches in philosophy of science. We survey recent work having adopted each of our modelling approaches and address the importance of computational modelling in language acquisition studies.},
	language = {en},
	number = {6},
	urldate = {2024-11-01},
	journal = {Language and Linguistics Compass},
	author = {Portelance, Eva and Jasbi, Masoud},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.70001},
	keywords = {cognitive models, instrumentalism, language acquisition, language models, neural networks, proof of concept, realism},
	pages = {e70001},
}

@misc{liu_mind_2024,
	title = {Mind {Your} {Step} (by {Step}): {Chain}-of-{Thought} can reduce performance on tasks where thinking makes humans worse},
	shorttitle = {Mind {Your} {Step} (by {Step})},
	url = {http://arxiv.org/abs/2410.21333},
	doi = {10.48550/arXiv.2410.21333},
	abstract = {Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, we find that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3\% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. We also identify three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, our results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help us identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, we offer a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning.},
	urldate = {2024-11-02},
	publisher = {arXiv},
	author = {Liu, Ryan and Geng, Jiayi and Wu, Addison J. and Sucholutsky, Ilia and Lombrozo, Tania and Griffiths, Thomas L.},
	month = oct,
	year = {2024},
	note = {arXiv:2410.21333 
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@article{forney_viterbi_1973,
	title = {The viterbi algorithm},
	volume = {61},
	issn = {1558-2256},
	url = {https://ieeexplore.ieee.org/document/1450960},
	doi = {10.1109/PROC.1973.9030},
	abstract = {The Viterbi algorithm (VA) is a recursive optimal solution to the problem of estimating the state sequence of a discrete-time finite-state Markov process observed in memoryless noise. Many problems in areas such as digital communications can be cast in this form. This paper gives a tutorial exposition of the algorithm and of how it is implemented and analyzed. Applications to date are reviewed. Increasing use of the algorithm in a widening variety of areas is foreseen.},
	number = {3},
	urldate = {2024-11-01},
	journal = {Proceedings of the IEEE},
	author = {Forney, G.D.},
	month = mar,
	year = {1973},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Algorithm design and analysis, Convolutional codes, Decoding, Digital communication, Markov processes, Recursive estimation, State estimation, Stochastic processes, Viterbi algorithm},
	pages = {268--278},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	doi = {10.48550/arXiv.1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2024-11-01},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-11-01},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980},
	keywords = {Computer Science - Machine Learning},
}

@article{mckenzie_inverse_2023,
	title = {Inverse {Scaling}: {When} {Bigger} {Isn}'t {Better}},
	issn = {2835-8856},
	shorttitle = {Inverse {Scaling}},
	url = {https://openreview.net/forum?id=DwgRm72GQF},
	abstract = {Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning datasets at inversescaling.com/data to allow for further investigation of inverse scaling. Our tasks have helped drive the discovery of U-shaped and inverted-U scaling trends, where an initial trend reverses, suggesting that scaling trends are less reliable at predicting the behavior of larger-scale models than previously understood. Overall, our results suggest that there are tasks for which increased model scale alone may not lead to progress, and that more careful thought needs to go into the data and objectives for training language models.},
	language = {en},
	urldate = {2024-10-18},
	journal = {Transactions on Machine Learning Research},
	author = {McKenzie, Ian R. and Lyzhov, Alexander and Pieler, Michael Martin and Parrish, Alicia and Mueller, Aaron and Prabhu, Ameya and McLean, Euan and Shen, Xudong and Cavanagh, Joe and Gritsevskiy, Andrew George and Kauffman, Derik and Kirtland, Aaron T. and Zhou, Zhengping and Zhang, Yuhui and Huang, Sicong and Wurgaft, Daniel and Weiss, Max and Ross, Alexis and Recchia, Gabriel and Liu, Alisa and Liu, Jiacheng and Tseng, Tom and Korbak, Tomasz and Kim, Najoung and Bowman, Samuel R. and Perez, Ethan},
	month = jun,
	year = {2023},
}

@misc{linzen_what_2018,
	title = {What can linguistics and deep learning contribute to each other?},
	url = {http://arxiv.org/abs/1809.04179},
	doi = {10.48550/arXiv.1809.04179},
	abstract = {Joe Pater's target article calls for greater interaction between neural network research and linguistics. I expand on this call and show how such interaction can benefit both fields. Linguists can contribute to research on neural networks for language technologies by clearly delineating the linguistic capabilities that can be expected of such systems, and by constructing controlled experimental paradigms that can determine whether those desiderata have been met. In the other direction, neural networks can benefit the scientific study of language by providing infrastructure for modeling human sentence processing and for evaluating the necessity of particular innate constraints on language acquisition.},
	urldate = {2024-10-18},
	publisher = {arXiv},
	author = {Linzen, Tal},
	month = sep,
	year = {2018},
	note = {arXiv:1809.04179},
	keywords = {Computer Science - Computation and Language},
}

@article{grant_how_2017,
	title = {How can memory-augmented neural networks pass a false-belief task?},
	volume = {39},
	url = {https://escholarship.org/uc/item/7zd0410m},
	abstract = {A question-answering system needs to be able to reason aboutunobserved causes in order to answer questions of the sort thatpeople face in everyday conversations. Recent neural networkmodels that incorporate explicit memory and attention mecha-nisms have taken steps towards this capability. However, thesemodels have not been tested in scenarios for which reasoningabout the unobservable mental states of other agents is nec-essary to answer a question. We propose a new set of tasksinspired by the well-known false-belief test to examine howa recent question-answering model performs in situations thatrequire reasoning about latent mental states. We find that themodel is only successful when the training and test data bearsubstantial similarity, as it memorizes how to answer specificquestions and cannot reason about the causal relationship be-tween actions and latent mental states. We introduce an ex-tension to the model that explicitly simulates the mental rep-resentations of different participants in a reasoning task, andshow that this capacity increases the model’s performance onour theory of mind test.},
	language = {en},
	number = {0},
	urldate = {2024-10-06},
	journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
	author = {Grant, Erin and Nematzadeh, Aida and Griffiths, Thomas L.},
	year = {2017},
}

@article{wang_knowledge_2014,
	title = {Knowledge graph embedding by translating on hyperplanes},
	volume = {28},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/8870},
	doi = {10.1609/aaai.v28i1.8870},
	abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.},
	language = {en},
	number = {1},
	urldate = {2024-10-18},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
	month = jun,
	year = {2014},
	note = {Number: 1},
	keywords = {TransH},
}

@inproceedings{li_language_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Language models are poor learners of directional inference},
	url = {https://aclanthology.org/2022.findings-emnlp.64},
	doi = {10.18653/v1/2022.findings-emnlp.64},
	abstract = {We examine LMs' competence of directional predicate entailments by supervised fine-tuning with prompts. Our analysis shows that contrary to their apparent success on standard NLI, LMs show limited ability to learn such directional inference; moreover, existing datasets fail to test directionality, and/or are infested by artefacts that can be learnt as proxy for entailments, yielding over-optimistic results. In response, we present BoOQA (Boolean Open QA), a robust multi-lingual evaluation benchmark for directional predicate entailments, extrinsic to existing training sets. On BoOQA, we establish baselines and show evidence of existing LM-prompting models being incompetent directional entailment learners, in contrast to entailment graphs, however limited by sparsity.},
	urldate = {2024-10-18},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Li, Tianyi and Hosseini, Mohammad Javad and Weber, Sabine and Steedman, Mark},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {903--921},
}

@article{portelance_learning_2024,
	title = {Learning the meanings of function words from grounded language using a visual question answering model},
	volume = {48},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13448},
	doi = {10.1111/cogs.13448},
	abstract = {Interpreting a seemingly simple function word like “or,” “behind,” or “more” can require logical, numerical, and relational reasoning. How are such words learned by children? Prior acquisition theories have often relied on positing a foundation of innate knowledge. Yet recent neural-network-based visual question answering models apparently can learn to use function words as part of answering questions about complex visual scenes. In this paper, we study what these models learn about function words, in the hope of better understanding how the meanings of these words can be learned by both models and children. We show that recurrent models trained on visually grounded language learn gradient semantics for function words requiring spatial and numerical reasoning. Furthermore, we find that these models can learn the meanings of logical connectives and and or without any prior knowledge of logical reasoning as well as early evidence that they are sensitive to alternative expressions when interpreting language. Finally, we show that word learning difficulty is dependent on the frequency of models' input. Our findings offer proof-of-concept evidence that it is possible to learn the nuanced interpretations of function words in a visually grounded context by using non-symbolic general statistical learning algorithms, without any prior knowledge of linguistic meaning.},
	language = {en},
	number = {5},
	urldate = {2024-10-18},
	journal = {Cognitive Science},
	author = {Portelance, Eva and Frank, Michael C. and Jurafsky, Dan},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13448},
	keywords = {Function word acquisition, Logical reasoning, Multimodal statistical learning, Neural network models, Proof of concept, Reasoning skills, Visual question answering, Visually-grounded language},
	pages = {e13448},
}

@misc{tanzer_benchmark_2024,
	title = {A {Benchmark} for learning to translate a new language from one grammar book},
	url = {http://arxiv.org/abs/2309.16575},
	doi = {10.48550/arXiv.2309.16575},
	abstract = {Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang -- a language with less than 200 speakers and therefore virtually no presence on the web -- using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials. We hope that MTOB will help measure LLM capabilities along a new dimension, and that the methods developed to solve it could help expand access to language technology for underserved communities by leveraging qualitatively different kinds of data than traditional machine translation.},
	urldate = {2024-10-18},
	publisher = {arXiv},
	author = {Tanzer, Garrett and Suzgun, Mirac and Visser, Eline and Jurafsky, Dan and Melas-Kyriazi, Luke},
	month = feb,
	year = {2024},
	note = {arXiv:2309.16575},
	keywords = {Computer Science - Computation and Language},
}

@article{li_inference-time_2023,
	title = {Inference-time intervention: eliciting truthful answers from a language model},
	volume = {36},
	shorttitle = {Inference-{Time} {Intervention}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html},
	language = {en},
	urldate = {2024-10-06},
	journal = {Advances in Neural Information Processing Systems},
	author = {Li, Kenneth and Patel, Oam and Viégas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
	month = dec,
	year = {2023},
	pages = {41451--41530},
}

@article{premack_does_1978,
	title = {Does the chimpanzee have a theory of mind?},
	volume = {1},
	issn = {1469-1825, 0140-525X},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/does-the-chimpanzee-have-a-theory-of-mind/1E96B02CD9850016B7C93BC6D2FEF1D0},
	doi = {10.1017/S0140525X00076512},
	abstract = {An individual has a theory of mind if he imputes mental states to himself and others. A system of inferences of this kind is properly viewed as a theory because such states are not directly observable, and the system can be used to make predictions about the behavior of others. As to the mental states the chimpanzee may infer, consider those inferred by our own species, for example, purpose or intention, as well as knowledge, belief, thinking, doubt, guessing, pretending, liking, and so forth. To determine whether or not the chimpanzee infers states of this kind, we showed an adult chimpanzee a series of videotaped scenes of a human actor struggling with a variety of problems. Some problems were simple, involving inaccessible food – bananas vertically or horizontally out of reach, behind a box, and so forth – as in the original Kohler problems; others were more complex, involving an actor unable to extricate himself from a locked cage, shivering because of a malfunctioning heater, or unable to play a phonograph because it was unplugged. With each videotape the chimpanzee was given several photographs, one a solution to the problem, such as a stick for the inaccessible bananas, a key for the locked up actor, a lit wick for the malfunctioning heater. The chimpanzee's consistent choice of the correct photographs can be understood by assuming that the animal recognized the videotape as representing a problem, understood the actor's purpose, and chose alternatives compatible with that purpose.},
	language = {en},
	number = {4},
	urldate = {2024-10-06},
	journal = {Behavioral and Brain Sciences},
	author = {Premack, David and Woodruff, Guy},
	month = dec,
	year = {1978},
	keywords = {chimpanzee communication, cognition, consciousness, intentionality, language, mind, primate intelligence.},
	pages = {515--526},
}

@article{grant_how_nodate,
	title = {How {Can} {Memory}-{Augmented} {Neural} {Networks} {Pass} a {False}-{Belief} {Task}?},
	abstract = {A question-answering system needs to be able to reason about unobserved causes in order to answer questions of the sort that people face in everyday conversations. Recent neural network models that incorporate explicit memory and attention mechanisms have taken steps towards this capability. However, these models have not been tested in scenarios for which reasoning about the unobservable mental states of other agents is necessary to answer a question. We propose a new set of tasks inspired by the well-known false-belief test to examine how a recent question-answering model performs in situations that require reasoning about latent mental states. We ﬁnd that the model is only successful when the training and test data bear substantial similarity, as it memorizes how to answer speciﬁc questions and cannot reason about the causal relationship between actions and latent mental states. We introduce an extension to the model that explicitly simulates the mental representations of different participants in a reasoning task, and show that this capacity increases the model’s performance on our theory of mind test.},
	language = {en},
	author = {Grant, Erin and Nematzadeh, Aida and Grifﬁths, Thomas L},
}

@inproceedings{qiu_towards_2022,
	address = {Edinburgh, UK},
	title = {Towards socially intelligent agents with mental state transition and human value},
	url = {https://aclanthology.org/2022.sigdial-1.16},
	doi = {10.18653/v1/2022.sigdial-1.16},
	abstract = {Building a socially intelligent agent involves many challenges. One of which is to track the agent's mental state transition and teach the agent to make decisions guided by its value like a human. Towards this end, we propose to incorporate mental state simulation and value modeling into dialogue agents. First, we build a hybrid mental state parser that extracts information from both the dialogue and event observations and maintains a graphical representation of the agent's mind; Meanwhile, the transformer-based value model learns human preferences from the human value dataset, ValueNet. Empirical results show that the proposed model attains state-of-the-art performance on the dialogue/action/emotion prediction task in the fantasy text-adventure game dataset, LIGHT. We also show example cases to demonstrate: (i) how the proposed mental state parser can assist the agent's decision by grounding on the context like locations and objects, and (ii) how the value model can help the agent make decisions based on its personal priorities.},
	urldate = {2024-09-20},
	booktitle = {Proceedings of the 23rd {Annual} {Meeting} of the {Special} {Interest} {Group} on {Discourse} and {Dialogue}},
	publisher = {Association for Computational Linguistics},
	author = {Qiu, Liang and Zhao, Yizhou and Liang, Yuan and Lu, Pan and Shi, Weiyan and Yu, Zhou and Zhu, Song-Chun},
	editor = {Lemon, Oliver and Hakkani-Tur, Dilek and Li, Junyi Jessy and Ashrafzadeh, Arash and Garcia, Daniel Hernández and Alikhani, Malihe and Vandyke, David and Dušek, Ondřej},
	month = sep,
	year = {2022},
	pages = {146--158},
}

@inproceedings{sclar_symmetric_2022,
	title = {Symmetric machine theory of mind},
	url = {https://proceedings.mlr.press/v162/sclar22a.html},
	abstract = {Theory of mind, the ability to model others’ thoughts and desires, is a cornerstone of human social intelligence. This makes it an important challenge for the machine learning community, but previous works mainly attempt to design agents that model the "mental state" of others as passive observers or in specific predefined roles, such as in speaker-listener scenarios. In contrast, we propose to model machine theory of mind in a more general symmetric scenario. We introduce a multi-agent environment SymmToM where, like in real life, all agents can speak, listen, see other agents, and move freely through the world. Effective strategies to maximize an agent’s reward require it to develop a theory of mind. We show that reinforcement learning agents that model the mental states of others achieve significant performance improvements over agents with no such theory of mind model. Importantly, our best agents still fail to achieve performance comparable to agents with access to the gold-standard mental state of other agents, demonstrating that the modeling of theory of mind in multi-agent scenarios is very much an open challenge.},
	language = {en},
	urldate = {2024-09-20},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sclar, Melanie and Neubig, Graham and Bisk, Yonatan},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {19450--19466},
}

@misc{brown_language_2020-1,
	title = {Language models are few-shot learners},
	url = {https://arxiv.org/abs/2005.14165v4},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	language = {en},
	urldate = {2024-09-20},
	journal = {arXiv.org},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = may,
	year = {2020},
}

@misc{zhou_i_2023,
	title = {I cast detect thoughts: learning to converse and guide with intents and theory-of-mind in dungeons and dragons},
	shorttitle = {I {Cast} {Detect} {Thoughts}},
	url = {http://arxiv.org/abs/2212.10060},
	doi = {10.48550/arXiv.2212.10060},
	abstract = {We propose a novel task, G4C, to study teacher-student natural language interactions in a goal-driven and grounded environment. Dungeons and Dragons (D\&D), a role-playing game, provides an ideal setting to investigate such interactions. Here, the Dungeon Master (DM), i.e., the teacher, guides the actions of several players -- students, each with their own personas and abilities -- to achieve shared goals grounded in a fantasy world. Our approach is to decompose and model these interactions into (1) the DM's intent to guide players toward a given goal; (2) the DM's guidance utterance to the players expressing this intent; and (3) a theory-of-mind (ToM) model that anticipates the players' reaction to the guidance one turn into the future. We develop a novel reinforcement learning (RL) method for training a DM that generates guidance for players by rewarding utterances where the intent matches the ToM-anticipated player actions. Human and automated evaluations show that a DM trained to explicitly model intents and incorporate ToM of the players using RL generates better-quality guidance that is 3x more likely to fulfill the DM's intent than a vanilla natural language generation (NLG) approach.},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Zhou, Pei and Zhu, Andrew and Hu, Jennifer and Pujara, Jay and Ren, Xiang and Callison-Burch, Chris and Choi, Yejin and Ammanabrolu, Prithviraj},
	month = may,
	year = {2023},
	note = {arXiv:2212.10060 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{tian_theory_2024,
	address = {Mexico City, Mexico},
	title = {A theory guided scaffolding instruction framework for {LLM}-enabled metaphor reasoning},
	url = {https://aclanthology.org/2024.naacl-long.428},
	doi = {10.18653/v1/2024.naacl-long.428},
	abstract = {Metaphor detection is a challenging task in figurative language processing, which aims to distinguish between metaphorical and literal expressions in text. Existing methods tackle metaphor detection via training or fine-tuning discriminative models on labeled data. However, these approaches struggle to explain the underlying reasoning process behind the metaphorical/literal judgment. Recently, large language models (LLMs) have shown promise in language reasoning tasks. Although promising, LLM-based methods for metaphor detection and reasoning are still faced with the challenging issue of bringing the explainable concepts for metaphor reasoning and their linguistic manifestation. To fill this gap, we propose a novel Theory guided Scaffolding Instruction (TSI) framework that instructs an LLM to infer the underlying reasoning process of metaphor detection guided by metaphor theories for the first time. Our work is inspired by a pedagogical strategy called scaffolding instruction, which encourages educators to provide questioning and support as scaffolding so as to assist learners in constructing the understanding of pedagogical goals step by step. We first construct a metaphor knowledge graph grounded in metaphor theory which serves as the instructional structure to obtain a series of scaffolding questions, directing the LLM to incrementally generate the reasoning process for metaphor understanding through dialogue interactions. During this theory guided instruction process, we explore the LLM's mastery boundary and provide the relevant knowledge as scaffolding support when the question is beyond the LLM's capability. Experimental results verify that our method significantly outperforms both the LLM-based reasoning methods and the SOTA methods in metaphor detection, indicating the facilitation of metaphor and instruction theories in guiding LLM-based reasoning process.},
	urldate = {2024-09-20},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tian, Yuan and Xu, Nan and Mao, Wenji},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {7738--7755},
}

@article{floyd_conversation_2021,
	title = {Conversation and culture},
	volume = {50},
	issn = {0084-6570, 1545-4290},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-anthro-101819-110158},
	doi = {10.1146/annurev-anthro-101819-110158},
	abstract = {Conversation analysis is a method for the systematic study of interaction in terms of a sequential turn-taking system. Research in conversation analysis has traditionally focused on speakers of English, and it is still unclear to what extent the system observed in that research applies to conversation more generally around the world. However, as this method is now being applied to conversation in a broader range of languages, it is increasingly possible to address questions about the nature of interactional diversity across different speech communities. The approach of pragmatic typology first applies sequential analysis to conversation from different speech communities and then compares interactional patterns in ways analogous to how traditional linguistic typology compares morphosyntax. This article discusses contemporary literature in pragmatic typology, including single-language studies and multilanguage comparisons reflecting both qualitative and quantitative methods. This research finds that microanalysis of face-to-face interaction can identify both universal trends and culture-specific interactional tendencies.},
	language = {en},
	number = {Volume 50, 2021},
	urldate = {2024-09-21},
	journal = {Annual Review of Anthropology},
	author = {Floyd, Simeon},
	month = oct,
	year = {2021},
	note = {Publisher: Annual Reviews},
	pages = {219--240},
}

@book{lakoff_metaphors_2003,
	address = {Chicago, IL},
	title = {Metaphors we live by},
	isbn = {978-0-226-46801-3},
	url = {https://press.uchicago.edu/ucp/books/book/chicago/M/bo3637992.html},
	abstract = {The now-classic Metaphors We Live By changed our understanding of metaphor and its role in language and the mind. Metaphor, the authors explain, is a fundamental mechanism of mind, one that allows us to use what we know about our physical and social experience to provide understanding of countless other subjects. Because such metaphors structure our most basic understandings of our experience, they are "metaphors we live by"—metaphors that can shape our perceptions and actions without our ever noticing them.In this updated edition of Lakoff and Johnson’s influential book, the authors supply an afterword surveying how their theory of metaphor has developed within the cognitive sciences to become central to the contemporary understanding of how we think and how we express our thoughts in language.},
	language = {en},
	urldate = {2024-09-21},
	publisher = {University of Chicago Press},
	author = {Lakoff, George and Johnson, Mark},
	editor = {Afterword, With a new},
	month = apr,
	year = {2003},
	keywords = {categories, categorization, causation, cognition, concepts, culture, dehumanizing, experience, influence, language, linguistics, marginalization, metaphor, metonymy, nonfiction, objectivism, patterns, perception, personification, philosophy, politics, reality, similarity, sociology, subconscious, subjectivism, truth, understanding},
}

@incollection{wilks_preferential_2007,
	address = {Dordrecht},
	title = {A preferential, pattern-seeking, semantics for natural language inference},
	isbn = {978-1-4020-5285-9},
	url = {https://doi.org/10.1007/1-4020-5285-5_5},
	abstract = {The paper describes the way in which a Preference Semantics system for natural language analysis and generation tackles a difficult class of anaphoric inference problems: those requiring either analytic (conceptual) knowledge of a complex sort, or requiring weak inductive knowledge of the course of events in the real world. The method employed converts all available knowledge to a canonical template form and endeavors to create chains of non-deductive inferences from the unknowns to the possible referents. Its method for this is consistent with the overall principle of ‘‘semantic preference’’ used to set up the original meaning representation},
	language = {en},
	urldate = {2024-09-21},
	booktitle = {Words and {Intelligence} {I}: {Selected} {Papers} by {Yorick} {Wilks}},
	publisher = {Springer Netherlands},
	author = {Wilks, Yorick},
	editor = {Ahmad, Khurshid and Brewster, Christopher and Stevenson, Mark},
	year = {2007},
	doi = {10.1007/1-4020-5285-5_5},
	pages = {83--102},
}

@article{group_mip_2007,
	title = {{MIP}: a method for identifying metaphorically used words in discourse},
	volume = {22},
	issn = {1092-6488},
	shorttitle = {{MIP}},
	url = {https://www.tandfonline.com/doi/abs/10.1080/10926480709336752},
	doi = {10.1080/10926480709336752},
	abstract = {This article presents an explicit method that can be reliably employed to identify metaphorically used words in discourse. Our aim is to provide metaphor scholars with a tool that may be flexibly applied to many research contexts. We present the “metaphor identification procedure” (MIP), followed by an example of how the procedure can be applied to identifying metaphorically used words in 1 text. We then suggest a format for reporting the results of MIP, and present the data from our case study describing the empirical reliability of the procedure, discuss several complications associated with using the procedure in practice, and then briefly compare MIP to other proposals on metaphor identification. The final section of the paper suggests ways that MIP may be employed in disciplinary and interdisciplinary studies of metaphor.},
	number = {1},
	urldate = {2024-09-21},
	journal = {Metaphor and Symbol},
	author = {Group, Pragglejaz},
	month = jan,
	year = {2007},
	note = {Publisher: Routledge
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/10926480709336752},
	pages = {1--39},
}

@inproceedings{sap_neural_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Neural theory-of-mind? on the limits of social intelligence in large {LMs}},
	shorttitle = {Neural {Theory}-of-{Mind}?},
	url = {https://aclanthology.org/2022.emnlp-main.248},
	doi = {10.18653/v1/2022.emnlp-main.248},
	abstract = {Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theorybased perspective. We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models' ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55\% and 60\% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.},
	urldate = {2024-10-06},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Le Bras, Ronan and Fried, Daniel and Choi, Yejin},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {3762--3780},
}

@inproceedings{arodi_textual_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Textual time travel: a temporally informed approach to theory of mind},
	shorttitle = {Textual {Time} {Travel}},
	url = {https://aclanthology.org/2021.findings-emnlp.351},
	doi = {10.18653/v1/2021.findings-emnlp.351},
	abstract = {Natural language processing systems such as dialogue agents should be able to reason about other people's beliefs, intentions and desires. This capability, called theory of mind (ToM), is crucial, as it allows a model to predict and interpret the needs of users based on their mental states. A recent line of research evaluates the ToM capability of existing memory-augmented neural models through question-answering. These models perform poorly on false belief tasks where beliefs differ from reality, especially when the dataset contains distracting sentences. In this paper, we propose a new temporally informed approach for improving the ToM capability of memory-augmented neural models. Our model incorporates priors about the entities' minds and tracks their mental states as they evolve over time through an extended passage. It then responds to queries through textual time travel–i.e., by accessing the stored memory of an earlier time step. We evaluate our model on ToM datasets and find that this approach improves performance, particularly by correcting the predicted mental states to match the false belief.},
	urldate = {2024-10-06},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Arodi, Akshatha and Cheung, Jackie Chi Kit},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {4162--4172},
}

@inproceedings{nematzadeh_evaluating_2018,
	address = {Brussels, Belgium},
	title = {Evaluating theory of mind in question answering},
	url = {https://aclanthology.org/D18-1261},
	doi = {10.18653/v1/D18-1261},
	abstract = {We propose a new dataset for evaluating question answering models with respect to their capacity to reason about beliefs. Our tasks are inspired by theory-of-mind experiments that examine whether children are able to reason about the beliefs of others, in particular when those beliefs differ from reality. We evaluate a number of recent neural models with memory augmentation. We find that all fail on our tasks, which require keeping track of inconsistent states of the world; moreover, the models' accuracy decreases notably when random sentences are introduced to the tasks at test.},
	urldate = {2024-10-06},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Nematzadeh, Aida and Burns, Kaylee and Grant, Erin and Gopnik, Alison and Griffiths, Tom},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {2392--2400},
}

@inproceedings{lewis_retrieval-augmented_2020,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '20},
	title = {Retrieval-augmented generation for knowledge-intensive {NLP} tasks},
	isbn = {978-1-71382-954-6},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2024-10-04},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = dec,
	year = {2020},
	pages = {9459--9474},
}

@misc{sap_neural_2023,
	title = {Neural theory-of-mind? on the limits of social intelligence in large {LMs}},
	shorttitle = {Neural {Theory}-of-{Mind}?},
	url = {http://arxiv.org/abs/2210.13312},
	abstract = {Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.},
	language = {en},
	urldate = {2024-09-20},
	publisher = {arXiv},
	author = {Sap, Maarten and LeBras, Ronan and Fried, Daniel and Choi, Yejin},
	month = apr,
	year = {2023},
	note = {arXiv:2210.13312 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_221013312_nodate,
	title = {[2210.13312] {Neural} {Theory}-of-{Mind}? {On} the {Limits} of {Social} {Intelligence} in {Large} {LMs}},
	url = {https://arxiv.org/abs/2210.13312},
	urldate = {2024-09-20},
}

@article{nguyen_reassessing_2014,
	title = {Reassessing the bilingual advantage in theory of mind and its cognitive underpinnings},
	volume = {17},
	copyright = {Copyright © Cambridge University Press 2013},
	issn = {13667289},
	url = {https://www.proquest.com/llba/docview/1544319778/abstract/54B4BFCB9AC549BFPQ/2},
	doi = {10.1017/S1366728913000394},
	abstract = {The present study aimed (a) to determine whether the bilingual advantage in false-belief (FB) understanding is replicated when considering socio-economic status and (b) to assess whether conflict inhibition and/or working memory underpin the advantage, if there is one. Monolingual preschoolers (24 English monolinguals and 24 French monolinguals) and 24 English-French bilingual counterparts received FB, conflict inhibition, working memory, and verbal ability tests. Monolingual and bilingual groups were equivalent on parental income and education, measured through a parental questionnaire. Results indicated that bilinguals significantly outperformed monolinguals on FB, but only after statistically controlling for language proficiency and age. Working memory likely compensated for the potential negative impact of bilinguals' low language proficiency on FB. [PUBLICATION ABSTRACT]},
	language = {English},
	number = {2},
	urldate = {2024-09-18},
	journal = {Bilingualism},
	author = {Nguyen, Thien-Kim and Astington, Janet Wilde},
	month = apr,
	year = {2014},
	note = {Num Pages: 14
Place: Cambridge, United Kingdom
Publisher: Cambridge University Press},
	keywords = {Behavior, Bilingual education, Bilingualism, Children \& youth, Cognition \& reasoning, Cognitive development, Developmental psychology, English language, French language, Language, Language proficiency, Memory, Monolingualism, Multilingualism, Preschool children, Psychology, Questionnaires, Research methodology, Short term memory, Studies, Theory of mind, Working memory, bilingualism, cognitive inhibition, executive function, theory of mind},
	pages = {396--409},
}

@inproceedings{van_duijn_theory_2023,
	address = {Singapore},
	title = {Theory of mind in large language models: examining performance of 11 state-of-the-art models vs. children aged 7-10 on advanced tests},
	shorttitle = {Theory of {Mind} in {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.conll-1.25},
	doi = {10.18653/v1/2023.conll-1.25},
	abstract = {To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.},
	urldate = {2024-09-18},
	booktitle = {Proceedings of the 27th {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {van Duijn, Max and van Dijk, Bram and Kouwenhoven, Tom and de Valk, Werner and Spruit, Marco and van der Putten, Peter},
	editor = {Jiang, Jing and Reitter, David and Deng, Shumin},
	month = dec,
	year = {2023},
	pages = {389--402},
}

@inproceedings{sclar_minding_2023,
	address = {Toronto, Canada},
	title = {Minding language models' (lack of) theory of mind: a plug-and-play multi-character belief tracker},
	shorttitle = {Minding {Language} {Models}' ({Lack} of) {Theory} of {Mind}},
	url = {https://aclanthology.org/2023.acl-long.780},
	doi = {10.18653/v1/2023.acl-long.780},
	abstract = {Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity's beliefs, their estimation of other entities' beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks' theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset.},
	urldate = {2024-09-18},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sclar, Melanie and Kumar, Sachin and West, Peter and Suhr, Alane and Choi, Yejin and Tsvetkov, Yulia},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {13960--13980},
}

@inproceedings{warstadt_findings_2023,
	address = {Singapore},
	title = {Findings of the {BabyLM} challenge: sample-efficient pretraining on developmentally plausible corpora},
	shorttitle = {Findings of the {BabyLM} {Challenge}},
	url = {https://aclanthology.org/2023.conll-babylm.1},
	doi = {10.18653/v1/2023.conll-babylm.1},
	abstract = {Children can acquire language from less than 100 million words of input. Large language models are far less data-efficient: they typically require 3 or 4 orders of magnitude more data and still do not perform as well as humans on many evaluations. These intensive resource demands limit the ability of researchers to train new models and use existing models as developmentally plausible cognitive models. The BabyLM Challenge is a communal effort in which participants compete to optimize language model training on a fixed data budget. Submissions are compared on various evaluation tasks targeting grammatical ability, downstream task performance, and generalization. Participants can submit to up to three tracks with progressively looser data restrictions. From over 30 submissions, we extract concrete recommendations on how best to train data-efficient language models, and on where future efforts should (and perhaps should not) focus. The winning submissions using the LTG-BERT architecture (Samuel et al., 2023) outperformed models trained on trillions of words. Other submissions achieved strong results through training on shorter input sequences or training a student model on a pretrained teacher. Curriculum learning attempts, which accounted for a large number of submissions, were largely unsuccessful, though some showed modest improvements.},
	language = {en},
	urldate = {2024-09-18},
	booktitle = {Proceedings of the {BabyLM} {Challenge} at the 27th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	year = {2023},
	pages = {1--6},
}

@article{strachan_testing_2024,
	title = {Testing theory of mind in large language models and humans},
	volume = {8},
	copyright = {2024 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-024-01882-z},
	doi = {10.1038/s41562-024-01882-z},
	abstract = {At the core of what defines us as humans is the concept of theory of mind: the ability to track other people’s mental states. The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models exhibit behaviour that is indistinguishable from human behaviour in theory of mind tasks. Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities, from understanding false beliefs to interpreting indirect requests and recognizing irony and faux pas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a sample of 1,907 human participants. Across the battery of theory of mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas. Faux pas, however, was the only test where LLaMA2 outperformed humans. Follow-up manipulations of the belief likelihood revealed that the superiority of LLaMA2 was illusory, possibly reflecting a bias towards attributing ignorance. By contrast, the poor performance of GPT originated from a hyperconservative approach towards committing to conclusions rather than from a genuine failure of inference. These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.},
	language = {en},
	number = {7},
	urldate = {2024-09-18},
	journal = {Nature Human Behaviour},
	author = {Strachan, James W. A. and Albergo, Dalila and Borghini, Giulia and Pansardi, Oriana and Scaliti, Eugenio and Gupta, Saurabh and Saxena, Krati and Rufo, Alessandro and Panzeri, Stefano and Manzi, Guido and Graziano, Michael S. A. and Becchio, Cristina},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Technology},
	pages = {1285--1295},
}

@inproceedings{hu_fine-grained_2023,
	address = {Toronto, Canada},
	title = {A fine-grained comparison of pragmatic language understanding in humans and language models},
	url = {https://aclanthology.org/2023.acl-long.230},
	doi = {10.18653/v1/2023.acl-long.230},
	abstract = {Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations.},
	urldate = {2024-09-18},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hu, Jennifer and Floyd, Sammy and Jouravlev, Olessia and Fedorenko, Evelina and Gibson, Edward},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {4194--4213},
}

@inproceedings{bhardwaj_pre-training_2023,
	address = {Singapore},
	title = {Pre-training {LLMs} using human-like development data corpus},
	url = {https://aclanthology.org/2023.conll-babylm.30},
	doi = {10.18653/v1/2023.conll-babylm.30},
	urldate = {2024-09-18},
	booktitle = {Proceedings of the {BabyLM} {Challenge} at the 27th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Bhardwaj, Khushi and Shah, Raj Sanjay and Varma, Sashank},
	editor = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	month = dec,
	year = {2023},
	pages = {339--345},
}
